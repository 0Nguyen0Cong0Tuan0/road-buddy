{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0Nguyen0Cong0Tuan0/Road-Buddy-Challenge/blob/main/models/yolo_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-section"
      },
      "source": [
        "# **YOLO11 Fine-tuning for Traffic Object Detection**\n",
        "\n",
        "\n",
        "This notebook **fine-tunes YOLO11n and YOLO11l** on custom traffic datasets to improve detection of **road objects** such as cars, trucks, buses, lanes, traffic lights, road signs and **exclude unrelated** such as toothbrush, skis, wine glass, etc.\n",
        "\n",
        "\n",
        "\n",
        "**Datasets**\n",
        "\n",
        "| Dataset | Classes | Focus |\n",
        "|---------|---------|-------|\n",
        "| BDD100K | 12 | Vehicles, pedestrians, traffic signs/lights |\n",
        "| Road Lane v2 | 6 | Lane line types (dotted, solid, divider, etc.) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHXtP5CTgnPf",
        "outputId": "2ee2c73e-4bb4-43b2-c6b0-cb8de1433ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## **Setup & Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gn9CRB0Ewy2C"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "install-ultralytics"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AUCINCGw0RU",
        "outputId": "362798cf-d472-4beb-9f9e-b31831b6e205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bdd100k: bdd100k\n",
            "   YAML: data_yolo.yaml (found)\n",
            "   Classes: 12 - ['car', 'truck', 'bus', 'train', 'person']...\n",
            "road_lane: Road Lane.v2i.yolo26\n",
            "   YAML: data.yaml (found)\n",
            "   Classes: 6 - ['divider-line', 'dotted-line', 'double-line', 'random-line', 'road-sign-line']...\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'models' else Path.cwd()\n",
        "DATA_DIR = PROJECT_ROOT / 'drive' / 'MyDrive' / 'traffic datasets'\n",
        "MODELS_DIR = PROJECT_ROOT / 'drive' / 'MyDrive' / 'models'\n",
        "RUNS_DIR = PROJECT_ROOT / 'drive' / 'MyDrive' / 'runs'\n",
        "\n",
        "# Dataset paths\n",
        "DATASETS = {\n",
        "    'bdd100k': {\n",
        "        'path': DATA_DIR / 'bdd100k',\n",
        "        'yaml': DATA_DIR / 'bdd100k' / 'data_yolo.yaml',\n",
        "        'description': 'BDD100K - Vehicles, pedestrians, traffic objects',\n",
        "        'classes': ['car', 'truck', 'bus', 'train', 'person', 'rider',\n",
        "                   'bike', 'motor', 'traffic light', 'traffic sign', 'lane', 'drivable area']\n",
        "    },\n",
        "    'road_lane': {\n",
        "        'path': DATA_DIR / 'Road Lane.v2i.yolo26',\n",
        "        'yaml': DATA_DIR / 'Road Lane.v2i.yolo26' / 'data.yaml',\n",
        "        'description': 'Road Lane v2 - Lane line types',\n",
        "        'classes': ['divider-line', 'dotted-line', 'double-line',\n",
        "                   'random-line', 'road-sign-line', 'solid-line']\n",
        "    }\n",
        "}\n",
        "\n",
        "for name, info in DATASETS.items():\n",
        "    exists = info['path'].exists()\n",
        "    yaml_exists = info['yaml'].exists()\n",
        "    print(f\"{name}: {info['path'].name}\")\n",
        "    print(f\"   YAML: {info['yaml'].name} ({'found' if yaml_exists else 'NOT FOUND'})\")\n",
        "    print(f\"   Classes: {len(info['classes'])} - {info['classes'][:5]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzPJ-DGy5Dr9",
        "outputId": "0b0e2c2f-c0e9-4cfa-c55f-335d9ccb3397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Configuration\n",
            "   epochs: 50\n",
            "   imgsz: 640\n",
            "   batch: 16\n",
            "   patience: 10\n",
            "   device: cuda\n",
            "   workers: 4\n",
            "   save: True\n",
            "   plots: True\n",
            "   verbose: True\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    'epochs': 50,          # Number of training epochs\n",
        "    'imgsz': 640,          # Image size\n",
        "    'batch': 16,           # Batch size (reduce if GPU OOM)\n",
        "    'patience': 10,        # Early stopping patience\n",
        "    'device': DEVICE,      # Training device\n",
        "    'workers': 4,          # Data loader workers\n",
        "    'save': True,          # Save checkpoints\n",
        "    'plots': True,         # Generate training plots\n",
        "    'verbose': True,       # Verbose output\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\\nTraining Configuration\")\n",
        "\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L56W03Y5w8N"
      },
      "source": [
        "## **Fine-tuning on Road Lane Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4kNYwoI54mb"
      },
      "source": [
        "### **Train YOLO11n on Road Lane**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWqWzrgL5heD",
        "outputId": "a25c4445-bec5-4a4f-a6d7-3260bc502ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11n summary: 181 layers, 2,624,080 parameters, 0 gradients, 6.6 GFLOPs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(181, 2624080, 0, 6.614336)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_n_lane = YOLO('yolo11n.pt')\n",
        "model_n_lane.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwbXNsH458Cz",
        "outputId": "a0b06fb4-e5c2-483e-bd0a-82ca19f0854c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning YOLO11n on Road Lane Dataset\n",
            "Ultralytics 8.4.5 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolo11n_road_lane, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/runs/finetune, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/runs/finetune/yolo11n_road_lane, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 27.3MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    431842  ultralytics.nn.modules.head.Detect           [6, 16, None, [64, 128, 256]] \n",
            "YOLO11n summary: 182 layers, 2,591,010 parameters, 2,590,994 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.3MB 91.9MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 1.3Â±1.2 ms, read: 0.1Â±0.0 MB/s, size: 40.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/labels... 1400 images, 4 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1400/1400 2.6it/s 8:49\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/images/Dash_Cam_Owners_Indonesia_495_June_2023_mp4_5220_jpg.rf.50674e1a2ebe505d1458fcab49074175.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/images/Dash_Cam_Owners_Indonesia_495_June_2023_mp4_5220_jpg.rf.8d0d939f27016f9eb0957106d4c61822.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.1Â±1.6 ms, read: 0.1Â±0.0 MB/s, size: 52.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/valid/labels... 194 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 194/194 2.6it/s 1:15\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/valid/labels.cache\n",
            "Plotting labels to /content/drive/MyDrive/runs/finetune/yolo11n_road_lane/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.001, momentum=0.9) with parameter groups 0 weight(decay=0.0), 0 weight(decay=0.0005), 12 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/runs/finetune/yolo11n_road_lane\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      2.72G      1.404      3.843      1.207         91        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.2it/s 1:15\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.1s/it 8.0s\n",
            "                   all        194       1677      0.011      0.437      0.157       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50       2.9G      1.139      2.337     0.9851        146        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.7it/s 50.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.1s\n",
            "                   all        194       1677      0.913      0.215      0.252      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50       2.9G      1.017      1.618     0.9409        104        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.2it/s 5.9s\n",
            "                   all        194       1677      0.459      0.315      0.326      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      2.92G     0.9732      1.345      0.922        159        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.8it/s 50.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 3.9s\n",
            "                   all        194       1677      0.586      0.367      0.403      0.279\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      2.94G     0.9331      1.186     0.9061         79        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 44.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n",
            "                   all        194       1677      0.419      0.541       0.46      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      2.95G     0.9046      1.095     0.9039        112        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 44.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.1it/s 3.4s\n",
            "                   all        194       1677      0.719      0.449      0.514      0.378\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      2.96G     0.8941       1.06     0.8994         69        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 44.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n",
            "                   all        194       1677      0.654      0.514      0.585      0.425\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      2.98G     0.8746      0.979     0.8922        123        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n",
            "                   all        194       1677      0.617      0.554      0.608      0.437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50         3G      0.863     0.9533     0.8883        140        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.6it/s 4.4s\n",
            "                   all        194       1677      0.656      0.547      0.577      0.425\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      3.01G     0.8552     0.9336     0.8889        152        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.2it/s 3.1s\n",
            "                   all        194       1677      0.643      0.582      0.636      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      3.03G      0.849     0.8962     0.8848         73        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n",
            "                   all        194       1677      0.667      0.592      0.642      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      3.22G     0.8398     0.8758      0.884         79        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 44.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.7it/s 4.1s\n",
            "                   all        194       1677      0.718      0.521      0.623      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      3.24G      0.824     0.8428      0.882        141        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.5it/s 2.8s\n",
            "                   all        194       1677      0.733       0.64      0.688      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      3.25G     0.8109     0.8215     0.8777         89        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.8it/s 48.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.0it/s 3.6s\n",
            "                   all        194       1677       0.75      0.652       0.73       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      3.27G     0.8118     0.8003      0.877        105        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.2it/s 3.1s\n",
            "                   all        194       1677      0.736      0.633      0.721      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      3.29G     0.8031      0.779      0.874         96        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.8it/s 48.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 3.9s\n",
            "                   all        194       1677      0.789       0.62      0.721      0.532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50       3.3G     0.8047     0.7771     0.8764         65        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 1.8it/s 50.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.7it/s 4.2s\n",
            "                   all        194       1677      0.672      0.715      0.733      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      3.31G     0.7952     0.7611     0.8739         88        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.8it/s 2.5s\n",
            "                   all        194       1677      0.775      0.626      0.733      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      3.33G     0.7939     0.7539     0.8727        111        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n",
            "                   all        194       1677      0.751      0.686      0.755      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      3.34G     0.7898     0.7381     0.8743        114        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 3.8s\n",
            "                   all        194       1677      0.716      0.718      0.746      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      3.36G     0.7801     0.7264     0.8655        147        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n",
            "                   all        194       1677      0.795      0.699      0.754      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      3.37G     0.7862      0.728     0.8696        135        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.1it/s 42.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.6it/s 2.7s\n",
            "                   all        194       1677      0.746      0.636      0.719       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      3.39G     0.7736     0.7087     0.8653        110        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 3.9s\n",
            "                   all        194       1677      0.743      0.645      0.727      0.532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50       3.4G     0.7658     0.6926     0.8642        105        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n",
            "                   all        194       1677      0.701      0.669      0.731      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      3.42G     0.7692     0.6896     0.8647        125        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.1it/s 42.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.1s\n",
            "                   all        194       1677      0.735      0.693       0.74      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      3.43G      0.765     0.6941     0.8652        130        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 4.0s\n",
            "                   all        194       1677      0.739      0.693      0.749      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      3.45G     0.7599     0.6692      0.859         64        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n",
            "                   all        194       1677      0.724      0.731      0.762      0.559\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      3.46G     0.7513      0.658      0.861         86        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.3it/s 3.0s\n",
            "                   all        194       1677      0.749      0.731      0.785      0.579\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      3.48G     0.7633     0.6626     0.8632         54        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.7it/s 4.1s\n",
            "                   all        194       1677      0.799      0.716      0.765      0.557\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      3.49G     0.7468     0.6554     0.8634        117        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.1it/s 42.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 3.0s\n",
            "                   all        194       1677      0.758      0.722      0.765      0.559\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50       3.5G     0.7503     0.6499     0.8618        148        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.2it/s 3.1s\n",
            "                   all        194       1677      0.723      0.724      0.746      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      3.52G     0.7445     0.6403     0.8591        113        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 3.8s\n",
            "                   all        194       1677      0.787      0.665      0.763      0.558\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      3.54G     0.7407     0.6366     0.8605        106        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.1it/s 42.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 3.0s\n",
            "                   all        194       1677      0.724      0.724      0.758       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      3.55G     0.7422     0.6311     0.8593        106        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.1it/s 42.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n",
            "                   all        194       1677      0.795      0.722      0.764      0.559\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      3.56G     0.7316     0.6275     0.8578         71        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.1it/s 42.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.6it/s 4.3s\n",
            "                   all        194       1677      0.792      0.719      0.771      0.567\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50      3.58G     0.7404     0.6272     0.8568         66        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n",
            "                   all        194       1677      0.758      0.708      0.766      0.571\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      3.59G     0.7308     0.6187     0.8574        123        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.4it/s 2.9s\n",
            "                   all        194       1677      0.779      0.705      0.765      0.567\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      3.61G     0.7245      0.612     0.8546         97        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 2.0it/s 43.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.8it/s 3.8s\n",
            "                   all        194       1677      0.792      0.694       0.77      0.576\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 28, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "38 epochs completed in 0.524 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/finetune/yolo11n_road_lane/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/finetune/yolo11n_road_lane/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/runs/finetune/yolo11n_road_lane/weights/best.pt...\n",
            "Ultralytics 8.4.5 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 101 layers, 2,583,322 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.3it/s 5.2s\n",
            "                   all        194       1677      0.748      0.732      0.785       0.58\n",
            "          divider-line         33         40      0.695       0.75      0.733      0.545\n",
            "           dotted-line        168       1085      0.875      0.837      0.889      0.616\n",
            "           double-line          9         11      0.551      0.818      0.867      0.719\n",
            "           random-line         29        152      0.687      0.549      0.648      0.396\n",
            "        road-sign-line         22         39      0.868       0.59      0.675      0.453\n",
            "            solid-line        181        350      0.811      0.846      0.899      0.752\n",
            "Speed: 0.2ms preprocess, 2.9ms inference, 0.0ms loss, 7.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/runs/finetune/yolo11n_road_lane\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "print(\"Fine-tuning YOLO11n on Road Lane Dataset\")\n",
        "\n",
        "# Start training\n",
        "results_n_lane = model_n_lane.train(\n",
        "    data=str(DATASETS['road_lane']['yaml']),\n",
        "    project=str(RUNS_DIR / 'finetune'),\n",
        "    name='yolo11n_road_lane',\n",
        "    exist_ok=True,\n",
        "    **TRAINING_CONFIG\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBw2LlDB7sAE"
      },
      "source": [
        "### **Train YOLO11l on Road Lane**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy4qavfD6Jxo",
        "outputId": "2f34d36b-a1a2-44c8-9510-fc763d1205d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo11l.pt to 'yolo11l.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 49.0MB 329.7MB/s 0.1s\n",
            "YOLO11l summary: 357 layers, 25,372,160 parameters, 0 gradients, 87.6 GFLOPs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(357, 25372160, 0, 87.6134912)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_l_lane = YOLO('yolo11l.pt')\n",
        "model_l_lane.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxADWkhN71z7",
        "outputId": "264d3ae8-6084-4966-c1a1-106c7f671c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.4.5 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolo11l_road_lane, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/runs/finetune, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/runs/finetune/yolo11l_road_lane, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
            " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
            " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
            " 23        [16, 19, 22]  1   1415650  ultralytics.nn.modules.head.Detect           [6, 16, None, [256, 512, 512]]\n",
            "YOLO11l summary: 358 layers, 25,315,106 parameters, 25,315,090 gradients, 87.3 GFLOPs\n",
            "\n",
            "Transferred 1009/1015 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.7Â±0.3 ms, read: 21.5Â±8.3 MB/s, size: 39.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/labels.cache... 1400 images, 4 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1400/1400 367.0Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/images/Dash_Cam_Owners_Indonesia_495_June_2023_mp4_5220_jpg.rf.50674e1a2ebe505d1458fcab49074175.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/train/images/Dash_Cam_Owners_Indonesia_495_June_2023_mp4_5220_jpg.rf.8d0d939f27016f9eb0957106d4c61822.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 1.3Â±1.1 ms, read: 10.8Â±4.2 MB/s, size: 47.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/traffic datasets/Road Lane.v2i.yolo26/valid/labels.cache... 194 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 194/194 8.3Mit/s 0.0s\n",
            "Plotting labels to /content/drive/MyDrive/runs/finetune/yolo11l_road_lane/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.001, momentum=0.9) with parameter groups 0 weight(decay=0.0), 0 weight(decay=0.0005), 12 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/runs/finetune/yolo11l_road_lane\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      5.47G      1.229      2.338      1.227         86        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 1.5it/s 1:55\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 1.6it/s 8.1s\n",
            "                   all        194       1677       0.46      0.481      0.483      0.354\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50       6.5G     0.8073     0.8936     0.9145        134        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:29\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.5it/s 3.7s\n",
            "                   all        194       1677      0.705      0.591      0.641      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50       6.5G     0.7733     0.7665     0.9044         65        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:28\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.0it/s 4.4s\n",
            "                   all        194       1677      0.741      0.657       0.73      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50       6.6G     0.7596     0.6737     0.8913        114        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 1.9it/s 1:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.4it/s 3.8s\n",
            "                   all        194       1677      0.734      0.669      0.731      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      6.69G     0.7539      0.625      0.894        113        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.1it/s 4.2s\n",
            "                   all        194       1677      0.759      0.682      0.736      0.554\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      6.69G     0.7399     0.5861     0.8907         69        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:26\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.714      0.665      0.702      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      6.69G     0.7344     0.5717     0.8898        110        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.2it/s 4.1s\n",
            "                   all        194       1677       0.73      0.676      0.753      0.563\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      6.69G     0.7262     0.5419     0.8875        116        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.705      0.736      0.766      0.558\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      6.78G      0.716     0.5154     0.8746         88        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.5it/s 3.8s\n",
            "                   all        194       1677      0.706      0.723      0.758      0.565\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      6.78G     0.7037     0.5073     0.8774        147        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.822      0.686      0.778       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      6.78G     0.6994     0.4925     0.8744        109        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.4it/s 3.9s\n",
            "                   all        194       1677      0.751      0.692        0.7      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      6.78G     0.6933     0.4871     0.8742        133        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.7s\n",
            "                   all        194       1677      0.796      0.692      0.756      0.568\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      6.78G      0.686      0.468     0.8697        101        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.4it/s 3.8s\n",
            "                   all        194       1677      0.776      0.702      0.789      0.598\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      6.78G     0.6707     0.4605     0.8652         90        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:26\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.723      0.737      0.787       0.61\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      6.78G     0.6793      0.453     0.8651        164        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.0it/s 4.3s\n",
            "                   all        194       1677      0.729      0.723      0.796      0.591\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      6.78G     0.6654      0.443     0.8651        102        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.5it/s 3.8s\n",
            "                   all        194       1677      0.722      0.758      0.791      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      6.78G     0.6684     0.4385     0.8608        163        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.798      0.677      0.781      0.588\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      6.78G     0.6519     0.4288     0.8563         86        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.3it/s 3.9s\n",
            "                   all        194       1677      0.741       0.74      0.786      0.596\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      6.78G     0.6545      0.416     0.8558        108        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.2it/s 4.0s\n",
            "                   all        194       1677      0.785      0.715        0.8       0.62\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      6.78G     0.6407     0.4091     0.8541        105        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.0it/s 4.4s\n",
            "                   all        194       1677      0.823      0.756      0.818       0.62\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      6.78G     0.6452     0.4093     0.8506         59        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.754      0.746      0.793      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      6.78G      0.631     0.4002     0.8508        107        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.2it/s 4.0s\n",
            "                   all        194       1677      0.781      0.762      0.824      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      6.78G     0.6262     0.3934     0.8494         80        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.0it/s 1:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.795      0.719      0.799      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50      6.78G     0.6218     0.3893     0.8483         82        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.802      0.739      0.818      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      6.78G      0.615     0.3852     0.8454         76        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.0it/s 4.4s\n",
            "                   all        194       1677      0.801      0.681      0.813       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      6.78G     0.6083     0.3801     0.8415        104        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.7s\n",
            "                   all        194       1677      0.747      0.726      0.813      0.628\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      6.78G     0.6097      0.373     0.8398         52        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.5it/s 3.7s\n",
            "                   all        194       1677      0.769      0.751      0.818       0.62\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      6.78G     0.6056     0.3712     0.8406         84        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.3it/s 3.9s\n",
            "                   all        194       1677      0.767      0.777      0.825      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      6.78G     0.5902     0.3583     0.8373         74        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.1it/s 4.2s\n",
            "                   all        194       1677      0.842      0.697      0.815      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      6.78G     0.5901     0.3541     0.8357         79        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.796      0.736      0.821      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      6.78G     0.5888     0.3497     0.8358        173        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.838      0.682      0.809      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      6.78G     0.5924     0.3558     0.8344         65        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 175/175 2.1it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.6it/s 3.6s\n",
            "                   all        194       1677      0.826      0.724      0.803       0.62\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 22, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "32 epochs completed in 0.820 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/finetune/yolo11l_road_lane/weights/last.pt, 51.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/finetune/yolo11l_road_lane/weights/best.pt, 51.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/runs/finetune/yolo11l_road_lane/weights/best.pt...\n",
            "Ultralytics 8.4.5 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11l summary (fused): 191 layers, 25,283,938 parameters, 0 gradients, 86.6 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 1.8it/s 7.2s\n",
            "                   all        194       1677      0.781      0.762      0.823      0.631\n",
            "          divider-line         33         40      0.851      0.716      0.769       0.58\n",
            "           dotted-line        168       1085      0.923       0.82      0.925      0.674\n",
            "           double-line          9         11      0.428      0.818      0.747       0.65\n",
            "           random-line         29        152      0.798      0.623      0.742      0.471\n",
            "        road-sign-line         22         39      0.816      0.744      0.855      0.633\n",
            "            solid-line        181        350      0.869      0.851      0.901      0.779\n",
            "Speed: 0.5ms preprocess, 20.2ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/runs/finetune/yolo11l_road_lane\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Reduce batch size for larger model\n",
        "config_l = TRAINING_CONFIG.copy()\n",
        "config_l['batch'] = 8\n",
        "\n",
        "results_l_lane = model_l_lane.train(\n",
        "    data=str(DATASETS['road_lane']['yaml']),\n",
        "    project=str(RUNS_DIR / 'finetune'),\n",
        "    name='yolo11l_road_lane',\n",
        "    exist_ok=True,\n",
        "    **config_l\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTFNpQps78SX"
      },
      "source": [
        "## **Fine-tuning on BDD100K Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVKhAZ1m7-bN",
        "outputId": "1a0b0e6e-117d-440a-a544-ff1d0c23f1ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BDD100K YOLO labels found:\n",
            "   Train labels: 51166\n",
            "   Val labels: 10000\n"
          ]
        }
      ],
      "source": [
        "bdd_labels_dir = DATASETS['bdd100k']['path'] / 'yolo_labels'\n",
        "\n",
        "if bdd_labels_dir.exists():\n",
        "    train_labels = list((bdd_labels_dir / 'train').glob('*.txt')) if (bdd_labels_dir / 'train').exists() else []\n",
        "    val_labels = list((bdd_labels_dir / 'val').glob('*.txt')) if (bdd_labels_dir / 'val').exists() else []\n",
        "    print(f\"BDD100K YOLO labels found:\")\n",
        "    print(f\"   Train labels: {len(train_labels)}\")\n",
        "    print(f\"   Val labels: {len(val_labels)}\")\n",
        "else:\n",
        "    print(\"BDD100K YOLO labels not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e8zPBZTG8YWT",
        "outputId": "bf1225b4-5a2e-42e0-932f-e2cd2143c451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.4.5 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/traffic datasets/bdd100k/data_yolo.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolo11n_bdd100k, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/runs/finetune, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/runs/finetune/yolo11n_bdd100k, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=12\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    433012  ultralytics.nn.modules.head.Detect           [12, 16, None, [64, 128, 256]]\n",
            "YOLO11n summary: 182 layers, 2,592,180 parameters, 2,592,164 gradients, 6.5 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 234.7Â±522.9 ms, read: 0.1Â±0.0 MB/s, size: 46.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/traffic datasets/bdd100k/bdd100k/labels/100k/train... 0 images, 1126 backgrounds, 0 corrupt: 2% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1126/70053 1.9it/s 7:48<9:52:47\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36mget_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset_cache_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# attempt to load a *.cache file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDATASET_CACHE_VERSION\u001b[0m  \u001b[0;31m# matches current version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/utils.py\u001b[0m in \u001b[0;36mload_dataset_cache_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reduce pickle load time https://github.com/ultralytics/ultralytics/pull/1585\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/traffic datasets/bdd100k/bdd100k/labels/100k/train.cache'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3964696991.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_n_bdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolo11n.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m results_n_bdd = model_n_bdd.train(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bdd100k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yaml'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUNS_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'finetune'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_setup_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;31m# Dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         self.train_loader = self.get_dataloader(\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_RANK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Mode must be 'train' or 'val', not {mode}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch_distributed_zero_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# init dataset *.cache only once if DDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(self, img_path, mode, batch)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m     76\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munwrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_yolo_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36mbuild_yolo_dataset\u001b[0;34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOMultiModalDataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmulti_modal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mYOLODataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     return dataset(\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, task, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_segments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Can not use both segments and keypoints.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcache_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./labels.cache\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction, channels)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_img_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# single_cls and include_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36mget_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mget_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# identical hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# run cache ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Display cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36mcache_labels\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m             \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTQDM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mim_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnm_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mnm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnm_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mnf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnf_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/utils/tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Fine-tune YOLO11n on BDD100K\n",
        "model_n_bdd = YOLO('yolo11n.pt')\n",
        "\n",
        "results_n_bdd = model_n_bdd.train(\n",
        "    data=str(DATASETS['bdd100k']['yaml']),\n",
        "    project=str(RUNS_DIR / 'finetune'),\n",
        "    name='yolo11n_bdd100k',\n",
        "    exist_ok=True,\n",
        "    **TRAINING_CONFIG\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFX35_e98Zae"
      },
      "outputs": [],
      "source": [
        "model_l_bdd = YOLO('yolo11l.pt')\n",
        "\n",
        "config_l = TRAINING_CONFIG.copy()\n",
        "config_l['batch'] = 8\n",
        "\n",
        "results_l_bdd = model_l_bdd.train(\n",
        "    data=str(DATASETS['bdd100k']['yaml']),\n",
        "    project=str(RUNS_DIR / 'finetune'),\n",
        "    name='yolo11l_bdd100k',\n",
        "    exist_ok=True,\n",
        "    **config_l\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5bSUfS58hnD"
      },
      "source": [
        "## **Load Fine-tuned Models for Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoMUptku8i2k"
      },
      "outputs": [],
      "source": [
        "def find_best_weights(run_dir):\n",
        "    \"\"\"Find the best.pt weights file in a training run directory.\"\"\"\n",
        "    weights_path = run_dir / 'weights' / 'best.pt'\n",
        "    if weights_path.exists():\n",
        "        return weights_path\n",
        "    return None\n",
        "\n",
        "# Collect all trained models\n",
        "FINETUNED_MODELS = {}\n",
        "\n",
        "model_runs = [\n",
        "    ('YOLO11n_RoadLane', RUNS_DIR / 'finetune' / 'yolo11n_road_lane'),\n",
        "    ('YOLO11l_RoadLane', RUNS_DIR / 'finetune' / 'yolo11l_road_lane'),\n",
        "    ('YOLO11n_BDD100K', RUNS_DIR / 'finetune' / 'yolo11n_bdd100k'),\n",
        "    ('YOLO11l_BDD100K', RUNS_DIR / 'finetune' / 'yolo11l_bdd100k'),\n",
        "]\n",
        "\n",
        "for name, run_dir in model_runs:\n",
        "    weights = find_best_weights(run_dir)\n",
        "    if weights:\n",
        "        try:\n",
        "            model = YOLO(weights)\n",
        "            FINETUNED_MODELS[name] = {\n",
        "                'model': model,\n",
        "                'path': weights,\n",
        "                'classes': list(model.names.values())\n",
        "            }\n",
        "            print(f\"{name}: Loaded ({len(model.names)} classes)\")\n",
        "        except Exception as e:\n",
        "            print(f\"{name}: Failed to load - {e}\")\n",
        "    else:\n",
        "        print(f\"{name}: Not trained yet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih_EneUU9Ofg"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_MODELS = {\n",
        "    'YOLO11n_Pretrained': YOLO('yolo11n.pt'),\n",
        "    'YOLO11l_Pretrained': YOLO('yolo11l.pt'),\n",
        "}\n",
        "\n",
        "print(\"Pre-trained models:\")\n",
        "for name, model in PRETRAINED_MODELS.items():\n",
        "    print(f\"   {name}: {len(model.names)} classes (COCO)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4RhC_xS9XpJ"
      },
      "source": [
        "## **Performance evaluation**\n",
        "\n",
        "Compare fine-tuned models against pre-trained models.\n",
        "\n",
        "### **Inference Speed Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nao0JeDW_L60"
      },
      "outputs": [],
      "source": [
        "def benchmark_inference(model, images, model_name, num_runs=3, warmup=2):\n",
        "    \"\"\"\n",
        "    Benchmark inference speed of a model.\n",
        "    \"\"\"\n",
        "    times = []\n",
        "    detections = []\n",
        "    confidences = []\n",
        "    class_counts = Counter()\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        if images:\n",
        "            _ = model(images[0], verbose=False)\n",
        "\n",
        "    # Benchmark\n",
        "    for _ in range(num_runs):\n",
        "        for img in images:\n",
        "            start = time.time()\n",
        "            results = model(img, verbose=False)\n",
        "            inference_time = (time.time() - start) * 1000\n",
        "            times.append(inference_time)\n",
        "\n",
        "            for r in results:\n",
        "                n_det = len(r.boxes)\n",
        "                detections.append(n_det)\n",
        "                if n_det > 0:\n",
        "                    confs = r.boxes.conf.cpu().numpy()\n",
        "                    confidences.extend(confs.tolist())\n",
        "                    for cls_id in r.boxes.cls.cpu().numpy().astype(int):\n",
        "                        class_counts[model.names[cls_id]] += 1\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'avg_time_ms': np.mean(times),\n",
        "        'std_time_ms': np.std(times),\n",
        "        'fps': 1000 / np.mean(times),\n",
        "        'avg_detections': np.mean(detections),\n",
        "        'avg_confidence': np.mean(confidences) if confidences else 0,\n",
        "        'class_counts': dict(class_counts),\n",
        "        'total_detections': sum(class_counts.values())\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0a639l_x4L"
      },
      "outputs": [],
      "source": [
        "# Get test images from Road Lane dataset\n",
        "test_images_lane = list((DATASETS['road_lane']['path'] / 'test' / 'images').glob('*.jpg'))[:20]\n",
        "print(f\"Road Lane test images: {len(test_images_lane)}\")\n",
        "\n",
        "# Get test images from BDD100K\n",
        "bdd_val_dir = DATASETS['bdd100k']['path'] / 'bdd100k' / 'images' / '100k' / 'val'\n",
        "test_images_bdd = list(bdd_val_dir.glob('*.jpg'))[:50] if bdd_val_dir.exists() else []\n",
        "print(f\"BDD100K test images: {len(test_images_bdd)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qGk0ZkN_kyS"
      },
      "outputs": [],
      "source": [
        "# Run benchmarks on Road Lane test set\n",
        "print(\"Benchmarking on Road Lane Test Set\")\n",
        "\n",
        "benchmark_results = {}\n",
        "\n",
        "if test_images_lane:\n",
        "    # Benchmark fine-tuned models\n",
        "    for name, info in FINETUNED_MODELS.items():\n",
        "        if 'RoadLane' in name:  # Only Road Lane models\n",
        "            print(f\"\\nBenchmarking {name}...\")\n",
        "            benchmark_results[name] = benchmark_inference(\n",
        "                info['model'], test_images_lane, name, num_runs=2\n",
        "            )\n",
        "\n",
        "    # Benchmark pre-trained models for comparison\n",
        "    for name, model in PRETRAINED_MODELS.items():\n",
        "        print(f\"\\nBenchmarking {name}...\")\n",
        "        benchmark_results[name] = benchmark_inference(\n",
        "            model, test_images_lane, name, num_runs=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tGRfuvgRPjE"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPERFORMANCE COMPARISON\")\n",
        "\n",
        "comparison_data = []\n",
        "for name, result in benchmark_results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Avg Time (ms)': f\"{result['avg_time_ms']:.1f}\",\n",
        "        'FPS': f\"{result['fps']:.1f}\",\n",
        "        'Avg Detections': f\"{result['avg_detections']:.1f}\",\n",
        "        'Avg Confidence': f\"{result['avg_confidence']:.2%}\",\n",
        "        'Total Detections': result['total_detections']\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(comparison_data)\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHgEEYc2SFKP"
      },
      "outputs": [],
      "source": [
        "# Visualize performance comparison\n",
        "if benchmark_results:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    models = list(benchmark_results.keys())\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
        "\n",
        "    # 1. Inference Time\n",
        "    ax = axes[0, 0]\n",
        "    times = [benchmark_results[m]['avg_time_ms'] for m in models]\n",
        "    bars = ax.bar(models, times, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Time (ms)')\n",
        "    ax.set_title('â±ï¸ Inference Time (lower is better)', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    for bar, t in zip(bars, times):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                f'{t:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 2. FPS\n",
        "    ax = axes[0, 1]\n",
        "    fps = [benchmark_results[m]['fps'] for m in models]\n",
        "    bars = ax.bar(models, fps, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('FPS')\n",
        "    ax.set_title('ğŸš€ Frames Per Second (higher is better)', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    for bar, f in zip(bars, fps):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                f'{f:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 3. Average Detections\n",
        "    ax = axes[1, 0]\n",
        "    dets = [benchmark_results[m]['avg_detections'] for m in models]\n",
        "    bars = ax.bar(models, dets, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Detections')\n",
        "    ax.set_title('ğŸ“¦ Avg Detections per Image', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # 4. Average Confidence\n",
        "    ax = axes[1, 1]\n",
        "    confs = [benchmark_results[m]['avg_confidence'] for m in models]\n",
        "    bars = ax.bar(models, confs, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Confidence')\n",
        "    ax.set_title('ğŸ¯ Average Confidence Score', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hClDS1ydS6fe"
      },
      "source": [
        "## **Detection quality test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wowJpoTSTQmf"
      },
      "outputs": [],
      "source": [
        "def analyze_detection_relevance(model, images, model_name):\n",
        "    \"\"\"\n",
        "    Analyze what classes the model detects and categorize them.\n",
        "    \"\"\"\n",
        "    traffic_detections = Counter()\n",
        "    non_traffic_detections = Counter()\n",
        "    other_detections = Counter()\n",
        "\n",
        "    for img in images:\n",
        "        results = model(img, verbose=False)\n",
        "        for r in results:\n",
        "            for cls_id in r.boxes.cls.cpu().numpy().astype(int):\n",
        "                class_name = model.names[cls_id]\n",
        "                if class_name.lower() in {c.lower() for c in TRAFFIC_CLASSES}:\n",
        "                    traffic_detections[class_name] += 1\n",
        "                elif class_name.lower() in {c.lower() for c in NON_TRAFFIC_CLASSES}:\n",
        "                    non_traffic_detections[class_name] += 1\n",
        "                else:\n",
        "                    other_detections[class_name] += 1\n",
        "\n",
        "    total = sum(traffic_detections.values()) + sum(non_traffic_detections.values()) + sum(other_detections.values())\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'traffic_detections': dict(traffic_detections),\n",
        "        'non_traffic_detections': dict(non_traffic_detections),\n",
        "        'other_detections': dict(other_detections),\n",
        "        'traffic_count': sum(traffic_detections.values()),\n",
        "        'non_traffic_count': sum(non_traffic_detections.values()),\n",
        "        'other_count': sum(other_detections.values()),\n",
        "        'total': total,\n",
        "        'traffic_ratio': sum(traffic_detections.values()) / max(total, 1)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0irBasFwTC3v"
      },
      "outputs": [],
      "source": [
        "TRAFFIC_CLASSES = {\n",
        "    'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle',\n",
        "    'person', 'traffic light', 'stop sign', 'parking meter',\n",
        "    'lane', 'drivable area', 'road', 'rider', 'motor', 'bike',\n",
        "    'divider-line', 'dotted-line', 'double-line', 'random-line',\n",
        "    'road-sign-line', 'solid-line', 'traffic sign'\n",
        "}\n",
        "\n",
        "NON_TRAFFIC_CLASSES = {\n",
        "    'toothbrush', 'hair drier', 'wine glass', 'cup', 'fork', 'knife',\n",
        "    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
        "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'couch', 'bed',\n",
        "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
        "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
        "    'baseball glove', 'skateboard', 'surfboard', 'tennis racket'\n",
        "}\n",
        "\n",
        "print(f\"Traffic-related classes: {len(TRAFFIC_CLASSES)}\")\n",
        "print(f\"Non-traffic classes: {len(NON_TRAFFIC_CLASSES)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVenncslUQ3x"
      },
      "outputs": [],
      "source": [
        "relevance_results = {}\n",
        "\n",
        "# Use Road Lane test images\n",
        "test_images = test_images_lane[:15] if test_images_lane else []\n",
        "\n",
        "if test_images:\n",
        "    # Analyze fine-tuned models\n",
        "    for name, info in FINETUNED_MODELS.items():\n",
        "        print(f\"\\nAnalyzing {name}...\")\n",
        "        relevance_results[name] = analyze_detection_relevance(\n",
        "            info['model'], test_images, name\n",
        "        )\n",
        "\n",
        "    # Analyze pre-trained models\n",
        "    for name, model in PRETRAINED_MODELS.items():\n",
        "        print(f\"\\nAnalyzing {name}...\")\n",
        "        relevance_results[name] = analyze_detection_relevance(\n",
        "            model, test_images, name\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqXt6N3LUV0Y"
      },
      "outputs": [],
      "source": [
        "# Display relevance results\n",
        "print(\"\\nDETECTION RELEVANCE SUMMARY\")\n",
        "\n",
        "for name, result in relevance_results.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"   Traffic-related detections: {result['traffic_count']}\")\n",
        "    print(f\"   Non-traffic detections: {result['non_traffic_count']}\")\n",
        "    print(f\"   Other detections: {result['other_count']}\")\n",
        "    print(f\"   Traffic Focus Ratio: {result['traffic_ratio']:.1%}\")\n",
        "\n",
        "    if result['traffic_detections']:\n",
        "        top_traffic = sorted(result['traffic_detections'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        print(f\"   Top traffic classes: {dict(top_traffic)}\")\n",
        "\n",
        "    if result['non_traffic_detections']:\n",
        "        print(f\"   Unwanted detections: {result['non_traffic_detections']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l05uXRatUdQA"
      },
      "outputs": [],
      "source": [
        "# Visualize detection relevance comparison\n",
        "if relevance_results:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    models = list(relevance_results.keys())\n",
        "\n",
        "    # Stacked bar chart: Traffic vs Non-Traffic detections\n",
        "    ax = axes[0]\n",
        "    traffic = [relevance_results[m]['traffic_count'] for m in models]\n",
        "    non_traffic = [relevance_results[m]['non_traffic_count'] for m in models]\n",
        "    other = [relevance_results[m]['other_count'] for m in models]\n",
        "\n",
        "    x = np.arange(len(models))\n",
        "    width = 0.6\n",
        "\n",
        "    ax.bar(x, traffic, width, label='Traffic', color='#2ecc71')\n",
        "    ax.bar(x, non_traffic, width, bottom=traffic, label='Non-Traffic', color='#e74c3c')\n",
        "    ax.bar(x, other, width, bottom=[t+n for t,n in zip(traffic, non_traffic)],\n",
        "           label='Other', color='#95a5a6')\n",
        "\n",
        "    ax.set_ylabel('Number of Detections')\n",
        "    ax.set_title('ğŸš— Detection Categories by Model', fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "\n",
        "    # Traffic Focus Ratio\n",
        "    ax = axes[1]\n",
        "    ratios = [relevance_results[m]['traffic_ratio'] * 100 for m in models]\n",
        "    colors = ['#2ecc71' if r > 80 else '#f39c12' if r > 50 else '#e74c3c' for r in ratios]\n",
        "    bars = ax.bar(models, ratios, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Traffic Focus Ratio (%)')\n",
        "    ax.set_title('Traffic Detection Focus (higher = better)', fontweight='bold')\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='Good (80%)')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for bar, ratio in zip(bars, ratios):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
        "                f'{ratio:.0f}%', ha='center', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeH79GvIVETV"
      },
      "source": [
        "**Visual comparison: pre-trained vs fine-tuned**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E00sCNLJVJ9S"
      },
      "outputs": [],
      "source": [
        "def compare_detections(image_path, models_dict, title=\"Detection Comparison\"):\n",
        "    \"\"\"\n",
        "    Compare detection results from multiple models on the same image.\n",
        "    \"\"\"\n",
        "    n_models = len(models_dict)\n",
        "    fig, axes = plt.subplots(1, n_models + 1, figsize=(5 * (n_models + 1), 5))\n",
        "\n",
        "    # Original image\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    axes[0].imshow(img_rgb)\n",
        "    axes[0].set_title('Original', fontsize=11)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Detection results from each model\n",
        "    for idx, (name, model) in enumerate(models_dict.items(), 1):\n",
        "        results = model(image_path, verbose=False)\n",
        "        annotated = results[0].plot()\n",
        "        annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        n_det = len(results[0].boxes)\n",
        "\n",
        "        # Get detected classes\n",
        "        detected_classes = []\n",
        "        if n_det > 0:\n",
        "            for cls_id in results[0].boxes.cls.cpu().numpy().astype(int):\n",
        "                detected_classes.append(model.names[cls_id])\n",
        "\n",
        "        axes[idx].imshow(annotated_rgb)\n",
        "        axes[idx].set_title(f'{name}\\n({n_det} detections)', fontsize=10)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnj2J9z3VRVU"
      },
      "outputs": [],
      "source": [
        "# Compare models on sample images\n",
        "print(\"ğŸ¨ Visual Detection Comparison\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Combine all models for comparison\n",
        "all_models = {}\n",
        "\n",
        "# Add fine-tuned models\n",
        "for name, info in FINETUNED_MODELS.items():\n",
        "    all_models[name] = info['model']\n",
        "\n",
        "# Add pre-trained models\n",
        "all_models.update(PRETRAINED_MODELS)\n",
        "\n",
        "# Show comparison on test images\n",
        "if test_images_lane:\n",
        "    for i, img_path in enumerate(test_images_lane[:3]):\n",
        "        print(f\"\\nğŸ“· Image {i+1}: {img_path.name}\")\n",
        "        compare_detections(img_path, all_models, f\"Sample {i+1}: {img_path.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Oa7vevVU-w"
      },
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ“‹ FINE-TUNING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nğŸ† TRAINED MODELS:\")\n",
        "for name, info in FINETUNED_MODELS.items():\n",
        "    print(f\"   âœ… {name}\")\n",
        "    print(f\"      Classes: {len(info['classes'])}\")\n",
        "    print(f\"      Path: {info['path']}\")\n",
        "\n",
        "print(\"\\nğŸ“Š PERFORMANCE HIGHLIGHTS:\")\n",
        "if benchmark_results:\n",
        "    # Find fastest and most accurate\n",
        "    fastest = min(benchmark_results.items(), key=lambda x: x[1]['avg_time_ms'])\n",
        "    most_detections = max(benchmark_results.items(), key=lambda x: x[1]['avg_detections'])\n",
        "\n",
        "    print(f\"   ğŸš€ Fastest: {fastest[0]} ({fastest[1]['fps']:.1f} FPS)\")\n",
        "    print(f\"   ğŸ“¦ Most detections: {most_detections[0]} ({most_detections[1]['avg_detections']:.1f} per image)\")\n",
        "\n",
        "print(\"\\nğŸ¯ DETECTION FOCUS:\")\n",
        "if relevance_results:\n",
        "    for name, result in relevance_results.items():\n",
        "        status = \"âœ…\" if result['traffic_ratio'] > 0.8 else \"âš ï¸\" if result['traffic_ratio'] > 0.5 else \"âŒ\"\n",
        "        print(f\"   {status} {name}: {result['traffic_ratio']:.1%} traffic-focused\")\n",
        "\n",
        "print(\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
        "print(\"\"\"\n",
        "   1. Use fine-tuned models for traffic detection - they're focused on\n",
        "      relevant objects and won't waste resources on irrelevant items.\n",
        "\n",
        "   2. YOLO11n is best for real-time applications (faster but less accurate)\n",
        "\n",
        "   3. YOLO11l is best when accuracy is critical (slower but more accurate)\n",
        "\n",
        "   4. The Road Lane model is specialized for lane detection\n",
        "\n",
        "   5. The BDD100K model covers broader traffic scenarios\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"âœ… Fine-tuning Complete!\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
