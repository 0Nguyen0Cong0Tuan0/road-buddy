{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöó YOLO11 Fine-tuning for Traffic Object Detection\n",
                "\n",
                "This notebook **fine-tunes YOLO11n and YOLO11l** on custom traffic datasets to improve detection of:\n",
                "- ‚úÖ **Road objects**: cars, trucks, buses, lanes, traffic lights, road signs\n",
                "- ‚ùå **Exclude unrelated**: toothbrush, skis, wine glass, etc.\n",
                "\n",
                "## üìö What This Notebook Does\n",
                "\n",
                "1. **Fine-tune YOLO11n** on custom traffic datasets\n",
                "2. **Fine-tune YOLO11l** on custom traffic datasets\n",
                "3. **Compare performance** (speed, accuracy, metrics)\n",
                "4. **Test detection quality** on traffic vs non-traffic objects\n",
                "5. **Export and save** the best models\n",
                "\n",
                "## üìÇ Datasets\n",
                "\n",
                "| Dataset | Classes | Focus |\n",
                "|---------|---------|-------|\n",
                "| BDD100K | 12 | Vehicles, pedestrians, traffic signs/lights |\n",
                "| Road Lane v2 | 6 | Lane line types (dotted, solid, divider, etc.) |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'models' else Path.cwd()\n",
                "DATA_DIR = PROJECT_ROOT / 'data' / 'custom train data'\n",
                "MODELS_DIR = PROJECT_ROOT / 'models'\n",
                "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
                "\n",
                "# Dataset paths\n",
                "DATASETS = {\n",
                "    'bdd100k': {\n",
                "        'path': DATA_DIR / 'bdd100k',\n",
                "        'yaml': DATA_DIR / 'bdd100k' / 'data_yolo.yaml',\n",
                "        'description': 'BDD100K - Vehicles, pedestrians, traffic objects',\n",
                "        'classes': ['car', 'truck', 'bus', 'train', 'person', 'rider', \n",
                "                   'bike', 'motor', 'traffic light', 'traffic sign', 'lane', 'drivable area']\n",
                "    },\n",
                "    'road_lane': {\n",
                "        'path': DATA_DIR / 'Road Lane.v2i.yolo26',\n",
                "        'yaml': DATA_DIR / 'Road Lane.v2i.yolo26' / 'data.yaml',\n",
                "        'description': 'Road Lane v2 - Lane line types',\n",
                "        'classes': ['divider-line', 'dotted-line', 'double-line', \n",
                "                   'random-line', 'road-sign-line', 'solid-line']\n",
                "    }\n",
                "}\n",
                "\n",
                "# Verify paths\n",
                "print(\"üìÇ Dataset Verification\")\n",
                "print(\"=\" * 60)\n",
                "for name, info in DATASETS.items():\n",
                "    exists = info['path'].exists()\n",
                "    yaml_exists = info['yaml'].exists()\n",
                "    status = \"‚úÖ\" if exists and yaml_exists else \"‚ùå\"\n",
                "    print(f\"{status} {name}: {info['path'].name}\")\n",
                "    print(f\"   YAML: {info['yaml'].name} ({'found' if yaml_exists else 'NOT FOUND'})\")\n",
                "    print(f\"   Classes: {len(info['classes'])} - {info['classes'][:5]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "TRAINING_CONFIG = {\n",
                "    'epochs': 50,          # Number of training epochs\n",
                "    'imgsz': 640,          # Image size\n",
                "    'batch': 16,           # Batch size (reduce if GPU OOM)\n",
                "    'patience': 10,        # Early stopping patience\n",
                "    'device': DEVICE,      # Training device\n",
                "    'workers': 4,          # Data loader workers\n",
                "    'save': True,          # Save checkpoints\n",
                "    'plots': True,         # Generate training plots\n",
                "    'verbose': True,       # Verbose output\n",
                "}\n",
                "\n",
                "# For quick testing, use fewer epochs\n",
                "QUICK_TEST = False  # Set to True for quick testing with fewer epochs\n",
                "if QUICK_TEST:\n",
                "    TRAINING_CONFIG['epochs'] = 5\n",
                "    TRAINING_CONFIG['patience'] = 3\n",
                "    print(\"‚ö†Ô∏è QUICK TEST MODE: Using reduced epochs\")\n",
                "\n",
                "print(\"\\n‚öôÔ∏è Training Configuration\")\n",
                "print(\"=\" * 60)\n",
                "for key, value in TRAINING_CONFIG.items():\n",
                "    print(f\"   {key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Fine-tuning on Road Lane Dataset\n",
                "\n",
                "First, we'll fine-tune on the **Road Lane v2** dataset which is already in YOLO format.\n",
                "\n",
                "### 2.1 Train YOLO11n on Road Lane"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load pre-trained YOLO11n\n",
                "print(\"üì• Loading YOLO11n pre-trained model...\")\n",
                "model_n_lane = YOLO('yolo11n.pt')\n",
                "print(\"‚úÖ YOLO11n loaded!\")\n",
                "\n",
                "# Display model info before training\n",
                "print(\"\\nüìä Pre-trained Model Info:\")\n",
                "model_n_lane.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fine-tune YOLO11n on Road Lane dataset\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"üöÄ Fine-tuning YOLO11n on Road Lane Dataset\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Start training\n",
                "results_n_lane = model_n_lane.train(\n",
                "    data=str(DATASETS['road_lane']['yaml']),\n",
                "    project=str(RUNS_DIR / 'finetune'),\n",
                "    name='yolo11n_road_lane',\n",
                "    exist_ok=True,\n",
                "    **TRAINING_CONFIG\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ YOLO11n Road Lane training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Train YOLO11l on Road Lane"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load pre-trained YOLO11l\n",
                "print(\"üì• Loading YOLO11l pre-trained model...\")\n",
                "model_l_lane = YOLO('yolo11l.pt')\n",
                "print(\"‚úÖ YOLO11l loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fine-tune YOLO11l on Road Lane dataset\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"üöÄ Fine-tuning YOLO11l on Road Lane Dataset\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Reduce batch size for larger model\n",
                "config_l = TRAINING_CONFIG.copy()\n",
                "config_l['batch'] = 8  # Smaller batch for larger model\n",
                "\n",
                "results_l_lane = model_l_lane.train(\n",
                "    data=str(DATASETS['road_lane']['yaml']),\n",
                "    project=str(RUNS_DIR / 'finetune'),\n",
                "    name='yolo11l_road_lane',\n",
                "    exist_ok=True,\n",
                "    **config_l\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ YOLO11l Road Lane training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Fine-tuning on BDD100K Dataset\n",
                "\n",
                "Now we'll fine-tune on the **BDD100K** dataset for multi-object detection.\n",
                "\n",
                "‚ö†Ô∏è **Note**: Make sure you've run the BDD100K to YOLO conversion script first!\n",
                "\n",
                "### 3.1 Train YOLO11n on BDD100K"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if BDD100K YOLO labels exist\n",
                "bdd_labels_dir = DATASETS['bdd100k']['path'] / 'yolo_labels'\n",
                "\n",
                "if bdd_labels_dir.exists():\n",
                "    train_labels = list((bdd_labels_dir / 'train').glob('*.txt')) if (bdd_labels_dir / 'train').exists() else []\n",
                "    val_labels = list((bdd_labels_dir / 'val').glob('*.txt')) if (bdd_labels_dir / 'val').exists() else []\n",
                "    print(f\"‚úÖ BDD100K YOLO labels found:\")\n",
                "    print(f\"   Train labels: {len(train_labels)}\")\n",
                "    print(f\"   Val labels: {len(val_labels)}\")\n",
                "    BDD_READY = len(train_labels) > 0\n",
                "else:\n",
                "    print(\"‚ùå BDD100K YOLO labels not found!\")\n",
                "    print(\"   Please run the convert_bdd100k_to_yolo.py script first.\")\n",
                "    BDD_READY = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fine-tune YOLO11n on BDD100K (only if labels are ready)\n",
                "if BDD_READY:\n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"üöÄ Fine-tuning YOLO11n on BDD100K Dataset\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    model_n_bdd = YOLO('yolo11n.pt')\n",
                "    \n",
                "    results_n_bdd = model_n_bdd.train(\n",
                "        data=str(DATASETS['bdd100k']['yaml']),\n",
                "        project=str(RUNS_DIR / 'finetune'),\n",
                "        name='yolo11n_bdd100k',\n",
                "        exist_ok=True,\n",
                "        **TRAINING_CONFIG\n",
                "    )\n",
                "    \n",
                "    print(\"\\n‚úÖ YOLO11n BDD100K training complete!\")\n",
                "else:\n",
                "    print(\"‚è≠Ô∏è Skipping BDD100K training (labels not ready)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fine-tune YOLO11l on BDD100K\n",
                "if BDD_READY:\n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"üöÄ Fine-tuning YOLO11l on BDD100K Dataset\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    model_l_bdd = YOLO('yolo11l.pt')\n",
                "    \n",
                "    config_l = TRAINING_CONFIG.copy()\n",
                "    config_l['batch'] = 8\n",
                "    \n",
                "    results_l_bdd = model_l_bdd.train(\n",
                "        data=str(DATASETS['bdd100k']['yaml']),\n",
                "        project=str(RUNS_DIR / 'finetune'),\n",
                "        name='yolo11l_bdd100k',\n",
                "        exist_ok=True,\n",
                "        **config_l\n",
                "    )\n",
                "    \n",
                "    print(\"\\n‚úÖ YOLO11l BDD100K training complete!\")\n",
                "else:\n",
                "    print(\"‚è≠Ô∏è Skipping BDD100K training (labels not ready)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Load Fine-tuned Models for Evaluation\n",
                "\n",
                "Load the best weights from training for evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find trained model weights\n",
                "def find_best_weights(run_dir):\n",
                "    \"\"\"Find the best.pt weights file in a training run directory.\"\"\"\n",
                "    weights_path = run_dir / 'weights' / 'best.pt'\n",
                "    if weights_path.exists():\n",
                "        return weights_path\n",
                "    return None\n",
                "\n",
                "# Collect all trained models\n",
                "FINETUNED_MODELS = {}\n",
                "\n",
                "model_runs = [\n",
                "    ('YOLO11n_RoadLane', RUNS_DIR / 'finetune' / 'yolo11n_road_lane'),\n",
                "    ('YOLO11l_RoadLane', RUNS_DIR / 'finetune' / 'yolo11l_road_lane'),\n",
                "    ('YOLO11n_BDD100K', RUNS_DIR / 'finetune' / 'yolo11n_bdd100k'),\n",
                "    ('YOLO11l_BDD100K', RUNS_DIR / 'finetune' / 'yolo11l_bdd100k'),\n",
                "]\n",
                "\n",
                "print(\"üì¶ Loading Fine-tuned Models\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for name, run_dir in model_runs:\n",
                "    weights = find_best_weights(run_dir)\n",
                "    if weights:\n",
                "        try:\n",
                "            model = YOLO(weights)\n",
                "            FINETUNED_MODELS[name] = {\n",
                "                'model': model,\n",
                "                'path': weights,\n",
                "                'classes': list(model.names.values())\n",
                "            }\n",
                "            print(f\"‚úÖ {name}: Loaded ({len(model.names)} classes)\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå {name}: Failed to load - {e}\")\n",
                "    else:\n",
                "        print(f\"‚è≥ {name}: Not trained yet\")\n",
                "\n",
                "print(f\"\\nüìä Total models loaded: {len(FINETUNED_MODELS)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Also load original pre-trained models for comparison\n",
                "PRETRAINED_MODELS = {\n",
                "    'YOLO11n_Pretrained': YOLO('yolo11n.pt'),\n",
                "    'YOLO11l_Pretrained': YOLO('yolo11l.pt'),\n",
                "}\n",
                "\n",
                "print(\"üì¶ Pre-trained Models (for comparison):\")\n",
                "for name, model in PRETRAINED_MODELS.items():\n",
                "    print(f\"   {name}: {len(model.names)} classes (COCO)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Performance Evaluation\n",
                "\n",
                "Compare fine-tuned models against pre-trained models.\n",
                "\n",
                "### 5.1 Inference Speed Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_inference(model, images, model_name, num_runs=3, warmup=2):\n",
                "    \"\"\"\n",
                "    Benchmark inference speed of a model.\n",
                "    \"\"\"\n",
                "    times = []\n",
                "    detections = []\n",
                "    confidences = []\n",
                "    class_counts = Counter()\n",
                "    \n",
                "    # Warmup\n",
                "    for _ in range(warmup):\n",
                "        if images:\n",
                "            _ = model(images[0], verbose=False)\n",
                "    \n",
                "    # Benchmark\n",
                "    for _ in range(num_runs):\n",
                "        for img in images:\n",
                "            start = time.time()\n",
                "            results = model(img, verbose=False)\n",
                "            inference_time = (time.time() - start) * 1000\n",
                "            times.append(inference_time)\n",
                "            \n",
                "            for r in results:\n",
                "                n_det = len(r.boxes)\n",
                "                detections.append(n_det)\n",
                "                if n_det > 0:\n",
                "                    confs = r.boxes.conf.cpu().numpy()\n",
                "                    confidences.extend(confs.tolist())\n",
                "                    for cls_id in r.boxes.cls.cpu().numpy().astype(int):\n",
                "                        class_counts[model.names[cls_id]] += 1\n",
                "    \n",
                "    return {\n",
                "        'model_name': model_name,\n",
                "        'avg_time_ms': np.mean(times),\n",
                "        'std_time_ms': np.std(times),\n",
                "        'fps': 1000 / np.mean(times),\n",
                "        'avg_detections': np.mean(detections),\n",
                "        'avg_confidence': np.mean(confidences) if confidences else 0,\n",
                "        'class_counts': dict(class_counts),\n",
                "        'total_detections': sum(class_counts.values())\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Benchmark function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get test images from Road Lane dataset\n",
                "test_images_lane = list((DATASETS['road_lane']['path'] / 'test' / 'images').glob('*.jpg'))[:20]\n",
                "print(f\"üì∑ Road Lane test images: {len(test_images_lane)}\")\n",
                "\n",
                "# Get test images from BDD100K (if available)\n",
                "bdd_val_dir = DATASETS['bdd100k']['path'] / 'bdd100k' / 'images' / '100k' / 'val'\n",
                "test_images_bdd = list(bdd_val_dir.glob('*.jpg'))[:20] if bdd_val_dir.exists() else []\n",
                "print(f\"üì∑ BDD100K test images: {len(test_images_bdd)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run benchmarks on Road Lane test set\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"üèÉ Benchmarking on Road Lane Test Set\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "benchmark_results = {}\n",
                "\n",
                "if test_images_lane:\n",
                "    # Benchmark fine-tuned models\n",
                "    for name, info in FINETUNED_MODELS.items():\n",
                "        if 'RoadLane' in name:  # Only Road Lane models\n",
                "            print(f\"\\n‚è±Ô∏è Benchmarking {name}...\")\n",
                "            benchmark_results[name] = benchmark_inference(\n",
                "                info['model'], test_images_lane, name, num_runs=2\n",
                "            )\n",
                "    \n",
                "    # Benchmark pre-trained models for comparison\n",
                "    for name, model in PRETRAINED_MODELS.items():\n",
                "        print(f\"\\n‚è±Ô∏è Benchmarking {name}...\")\n",
                "        benchmark_results[name] = benchmark_inference(\n",
                "            model, test_images_lane, name, num_runs=2\n",
                "        )\n",
                "\n",
                "print(\"\\n‚úÖ Benchmarking complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display benchmark results\n",
                "print(\"\\nüìä PERFORMANCE COMPARISON\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "comparison_data = []\n",
                "for name, result in benchmark_results.items():\n",
                "    comparison_data.append({\n",
                "        'Model': name,\n",
                "        'Avg Time (ms)': f\"{result['avg_time_ms']:.1f}\",\n",
                "        'FPS': f\"{result['fps']:.1f}\",\n",
                "        'Avg Detections': f\"{result['avg_detections']:.1f}\",\n",
                "        'Avg Confidence': f\"{result['avg_confidence']:.2%}\",\n",
                "        'Total Detections': result['total_detections']\n",
                "    })\n",
                "\n",
                "df_results = pd.DataFrame(comparison_data)\n",
                "print(df_results.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize performance comparison\n",
                "if benchmark_results:\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "    \n",
                "    models = list(benchmark_results.keys())\n",
                "    colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
                "    \n",
                "    # 1. Inference Time\n",
                "    ax = axes[0, 0]\n",
                "    times = [benchmark_results[m]['avg_time_ms'] for m in models]\n",
                "    bars = ax.bar(models, times, color=colors, edgecolor='black')\n",
                "    ax.set_ylabel('Time (ms)')\n",
                "    ax.set_title('‚è±Ô∏è Inference Time (lower is better)', fontweight='bold')\n",
                "    ax.tick_params(axis='x', rotation=45)\n",
                "    for bar, t in zip(bars, times):\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
                "                f'{t:.1f}', ha='center', va='bottom', fontsize=9)\n",
                "    \n",
                "    # 2. FPS\n",
                "    ax = axes[0, 1]\n",
                "    fps = [benchmark_results[m]['fps'] for m in models]\n",
                "    bars = ax.bar(models, fps, color=colors, edgecolor='black')\n",
                "    ax.set_ylabel('FPS')\n",
                "    ax.set_title('üöÄ Frames Per Second (higher is better)', fontweight='bold')\n",
                "    ax.tick_params(axis='x', rotation=45)\n",
                "    for bar, f in zip(bars, fps):\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
                "                f'{f:.1f}', ha='center', va='bottom', fontsize=9)\n",
                "    \n",
                "    # 3. Average Detections\n",
                "    ax = axes[1, 0]\n",
                "    dets = [benchmark_results[m]['avg_detections'] for m in models]\n",
                "    bars = ax.bar(models, dets, color=colors, edgecolor='black')\n",
                "    ax.set_ylabel('Detections')\n",
                "    ax.set_title('üì¶ Avg Detections per Image', fontweight='bold')\n",
                "    ax.tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    # 4. Average Confidence\n",
                "    ax = axes[1, 1]\n",
                "    confs = [benchmark_results[m]['avg_confidence'] for m in models]\n",
                "    bars = ax.bar(models, confs, color=colors, edgecolor='black')\n",
                "    ax.set_ylabel('Confidence')\n",
                "    ax.set_title('üéØ Average Confidence Score', fontweight='bold')\n",
                "    ax.tick_params(axis='x', rotation=45)\n",
                "    ax.set_ylim(0, 1)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Detection Quality Test: Traffic vs Non-Traffic Objects\n",
                "\n",
                "This is crucial: we want our fine-tuned models to:\n",
                "- ‚úÖ **Detect** traffic objects (cars, lanes, signs)\n",
                "- ‚ùå **NOT detect** irrelevant objects (toothbrush, wine glass, etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define traffic-related and unrelated classes\n",
                "TRAFFIC_CLASSES = {\n",
                "    'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle',\n",
                "    'person', 'traffic light', 'stop sign', 'parking meter',\n",
                "    'lane', 'drivable area', 'road', 'rider', 'motor', 'bike',\n",
                "    'divider-line', 'dotted-line', 'double-line', 'random-line', \n",
                "    'road-sign-line', 'solid-line', 'traffic sign'\n",
                "}\n",
                "\n",
                "NON_TRAFFIC_CLASSES = {\n",
                "    'toothbrush', 'hair drier', 'wine glass', 'cup', 'fork', 'knife',\n",
                "    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
                "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'couch', 'bed',\n",
                "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
                "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
                "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
                "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
                "    'baseball glove', 'skateboard', 'surfboard', 'tennis racket'\n",
                "}\n",
                "\n",
                "print(f\"üöó Traffic-related classes: {len(TRAFFIC_CLASSES)}\")\n",
                "print(f\"üö´ Non-traffic classes: {len(NON_TRAFFIC_CLASSES)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_detection_relevance(model, images, model_name):\n",
                "    \"\"\"\n",
                "    Analyze what classes the model detects and categorize them.\n",
                "    \"\"\"\n",
                "    traffic_detections = Counter()\n",
                "    non_traffic_detections = Counter()\n",
                "    other_detections = Counter()\n",
                "    \n",
                "    for img in images:\n",
                "        results = model(img, verbose=False)\n",
                "        for r in results:\n",
                "            for cls_id in r.boxes.cls.cpu().numpy().astype(int):\n",
                "                class_name = model.names[cls_id]\n",
                "                if class_name.lower() in {c.lower() for c in TRAFFIC_CLASSES}:\n",
                "                    traffic_detections[class_name] += 1\n",
                "                elif class_name.lower() in {c.lower() for c in NON_TRAFFIC_CLASSES}:\n",
                "                    non_traffic_detections[class_name] += 1\n",
                "                else:\n",
                "                    other_detections[class_name] += 1\n",
                "    \n",
                "    total = sum(traffic_detections.values()) + sum(non_traffic_detections.values()) + sum(other_detections.values())\n",
                "    \n",
                "    return {\n",
                "        'model_name': model_name,\n",
                "        'traffic_detections': dict(traffic_detections),\n",
                "        'non_traffic_detections': dict(non_traffic_detections),\n",
                "        'other_detections': dict(other_detections),\n",
                "        'traffic_count': sum(traffic_detections.values()),\n",
                "        'non_traffic_count': sum(non_traffic_detections.values()),\n",
                "        'other_count': sum(other_detections.values()),\n",
                "        'total': total,\n",
                "        'traffic_ratio': sum(traffic_detections.values()) / max(total, 1)\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Detection relevance analyzer defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze detection relevance for all models\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"üîç Analyzing Detection Relevance (Traffic vs Non-Traffic)\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "relevance_results = {}\n",
                "\n",
                "# Use Road Lane test images\n",
                "test_images = test_images_lane[:15] if test_images_lane else []\n",
                "\n",
                "if test_images:\n",
                "    # Analyze fine-tuned models\n",
                "    for name, info in FINETUNED_MODELS.items():\n",
                "        print(f\"\\nüìä Analyzing {name}...\")\n",
                "        relevance_results[name] = analyze_detection_relevance(\n",
                "            info['model'], test_images, name\n",
                "        )\n",
                "    \n",
                "    # Analyze pre-trained models\n",
                "    for name, model in PRETRAINED_MODELS.items():\n",
                "        print(f\"\\nüìä Analyzing {name}...\")\n",
                "        relevance_results[name] = analyze_detection_relevance(\n",
                "            model, test_images, name\n",
                "        )\n",
                "\n",
                "print(\"\\n‚úÖ Analysis complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display relevance results\n",
                "print(\"\\nüìä DETECTION RELEVANCE SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "for name, result in relevance_results.items():\n",
                "    print(f\"\\nüî∑ {name}\")\n",
                "    print(\"-\" * 50)\n",
                "    print(f\"   ‚úÖ Traffic-related detections: {result['traffic_count']}\")\n",
                "    print(f\"   ‚ùå Non-traffic detections: {result['non_traffic_count']}\")\n",
                "    print(f\"   ‚ùì Other detections: {result['other_count']}\")\n",
                "    print(f\"   üìà Traffic Focus Ratio: {result['traffic_ratio']:.1%}\")\n",
                "    \n",
                "    if result['traffic_detections']:\n",
                "        top_traffic = sorted(result['traffic_detections'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
                "        print(f\"   üöó Top traffic classes: {dict(top_traffic)}\")\n",
                "    \n",
                "    if result['non_traffic_detections']:\n",
                "        print(f\"   ‚ö†Ô∏è Unwanted detections: {result['non_traffic_detections']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize detection relevance comparison\n",
                "if relevance_results:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "    \n",
                "    models = list(relevance_results.keys())\n",
                "    \n",
                "    # Stacked bar chart: Traffic vs Non-Traffic detections\n",
                "    ax = axes[0]\n",
                "    traffic = [relevance_results[m]['traffic_count'] for m in models]\n",
                "    non_traffic = [relevance_results[m]['non_traffic_count'] for m in models]\n",
                "    other = [relevance_results[m]['other_count'] for m in models]\n",
                "    \n",
                "    x = np.arange(len(models))\n",
                "    width = 0.6\n",
                "    \n",
                "    ax.bar(x, traffic, width, label='Traffic ‚úÖ', color='#2ecc71')\n",
                "    ax.bar(x, non_traffic, width, bottom=traffic, label='Non-Traffic ‚ùå', color='#e74c3c')\n",
                "    ax.bar(x, other, width, bottom=[t+n for t,n in zip(traffic, non_traffic)], \n",
                "           label='Other', color='#95a5a6')\n",
                "    \n",
                "    ax.set_ylabel('Number of Detections')\n",
                "    ax.set_title('üöó Detection Categories by Model', fontweight='bold')\n",
                "    ax.set_xticks(x)\n",
                "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
                "    ax.legend()\n",
                "    \n",
                "    # Traffic Focus Ratio\n",
                "    ax = axes[1]\n",
                "    ratios = [relevance_results[m]['traffic_ratio'] * 100 for m in models]\n",
                "    colors = ['#2ecc71' if r > 80 else '#f39c12' if r > 50 else '#e74c3c' for r in ratios]\n",
                "    bars = ax.bar(models, ratios, color=colors, edgecolor='black')\n",
                "    ax.set_ylabel('Traffic Focus Ratio (%)')\n",
                "    ax.set_title('üéØ Traffic Detection Focus (higher = better)', fontweight='bold')\n",
                "    ax.set_ylim(0, 100)\n",
                "    ax.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='Good (80%)')\n",
                "    ax.tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    for bar, ratio in zip(bars, ratios):\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
                "                f'{ratio:.0f}%', ha='center', fontweight='bold')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Visual Comparison: Pre-trained vs Fine-tuned\n",
                "\n",
                "Let's see how the models perform on actual images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_detections(image_path, models_dict, title=\"Detection Comparison\"):\n",
                "    \"\"\"\n",
                "    Compare detection results from multiple models on the same image.\n",
                "    \"\"\"\n",
                "    n_models = len(models_dict)\n",
                "    fig, axes = plt.subplots(1, n_models + 1, figsize=(5 * (n_models + 1), 5))\n",
                "    \n",
                "    # Original image\n",
                "    img = cv2.imread(str(image_path))\n",
                "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "    axes[0].imshow(img_rgb)\n",
                "    axes[0].set_title('Original', fontsize=11)\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    # Detection results from each model\n",
                "    for idx, (name, model) in enumerate(models_dict.items(), 1):\n",
                "        results = model(image_path, verbose=False)\n",
                "        annotated = results[0].plot()\n",
                "        annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "        n_det = len(results[0].boxes)\n",
                "        \n",
                "        # Get detected classes\n",
                "        detected_classes = []\n",
                "        if n_det > 0:\n",
                "            for cls_id in results[0].boxes.cls.cpu().numpy().astype(int):\n",
                "                detected_classes.append(model.names[cls_id])\n",
                "        \n",
                "        axes[idx].imshow(annotated_rgb)\n",
                "        axes[idx].set_title(f'{name}\\n({n_det} detections)', fontsize=10)\n",
                "        axes[idx].axis('off')\n",
                "    \n",
                "    plt.suptitle(title, fontsize=13, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "print(\"‚úÖ Comparison function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare models on sample images\n",
                "print(\"üé® Visual Detection Comparison\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Combine all models for comparison\n",
                "all_models = {}\n",
                "\n",
                "# Add fine-tuned models\n",
                "for name, info in FINETUNED_MODELS.items():\n",
                "    all_models[name] = info['model']\n",
                "\n",
                "# Add pre-trained models\n",
                "all_models.update(PRETRAINED_MODELS)\n",
                "\n",
                "# Show comparison on test images\n",
                "if test_images_lane:\n",
                "    for i, img_path in enumerate(test_images_lane[:3]):\n",
                "        print(f\"\\nüì∑ Image {i+1}: {img_path.name}\")\n",
                "        compare_detections(img_path, all_models, f\"Sample {i+1}: {img_path.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Export Fine-tuned Models\n",
                "\n",
                "Save the best fine-tuned models to the models directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy best weights to models directory\n",
                "print(\"üì¶ Exporting Fine-tuned Models\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "for name, info in FINETUNED_MODELS.items():\n",
                "    src = info['path']\n",
                "    dst = MODELS_DIR / f\"{name.lower().replace('_', '-')}.pt\"\n",
                "    \n",
                "    try:\n",
                "        shutil.copy(src, dst)\n",
                "        print(f\"‚úÖ Saved: {dst.name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed to save {name}: {e}\")\n",
                "\n",
                "print(f\"\\nüìÅ Models saved to: {MODELS_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Summary & Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"üìã FINE-TUNING SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\nüèÜ TRAINED MODELS:\")\n",
                "for name, info in FINETUNED_MODELS.items():\n",
                "    print(f\"   ‚úÖ {name}\")\n",
                "    print(f\"      Classes: {len(info['classes'])}\")\n",
                "    print(f\"      Path: {info['path']}\")\n",
                "\n",
                "print(\"\\nüìä PERFORMANCE HIGHLIGHTS:\")\n",
                "if benchmark_results:\n",
                "    # Find fastest and most accurate\n",
                "    fastest = min(benchmark_results.items(), key=lambda x: x[1]['avg_time_ms'])\n",
                "    most_detections = max(benchmark_results.items(), key=lambda x: x[1]['avg_detections'])\n",
                "    \n",
                "    print(f\"   üöÄ Fastest: {fastest[0]} ({fastest[1]['fps']:.1f} FPS)\")\n",
                "    print(f\"   üì¶ Most detections: {most_detections[0]} ({most_detections[1]['avg_detections']:.1f} per image)\")\n",
                "\n",
                "print(\"\\nüéØ DETECTION FOCUS:\")\n",
                "if relevance_results:\n",
                "    for name, result in relevance_results.items():\n",
                "        status = \"‚úÖ\" if result['traffic_ratio'] > 0.8 else \"‚ö†Ô∏è\" if result['traffic_ratio'] > 0.5 else \"‚ùå\"\n",
                "        print(f\"   {status} {name}: {result['traffic_ratio']:.1%} traffic-focused\")\n",
                "\n",
                "print(\"\\nüí° RECOMMENDATIONS:\")\n",
                "print(\"\"\"\n",
                "   1. Use fine-tuned models for traffic detection - they're focused on\n",
                "      relevant objects and won't waste resources on irrelevant items.\n",
                "   \n",
                "   2. YOLO11n is best for real-time applications (faster but less accurate)\n",
                "   \n",
                "   3. YOLO11l is best when accuracy is critical (slower but more accurate)\n",
                "   \n",
                "   4. The Road Lane model is specialized for lane detection\n",
                "   \n",
                "   5. The BDD100K model covers broader traffic scenarios\n",
                "\"\"\")\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"‚úÖ Fine-tuning Complete!\")\n",
                "print(\"=\" * 70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
