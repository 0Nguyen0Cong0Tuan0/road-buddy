{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Frame Scorer Benchmark**\n",
                "\n",
                "## **Strategies Tested**\n",
                "1. **CLIP Scoring** - Question-Frame Similarity using OpenAI CLIP\n",
                "   - ViT-L/14 (best quality)\n",
                "   - ViT-B/32 (faster)\n",
                "2. **Multilingual CLIP** - Direct Vietnamese support\n",
                "3. **Detection Scoring** - YOLO-based object detection boost\n",
                "4. **Distinctiveness** - Inter-Frame Distinctiveness (IFD)\n",
                "5. **Combined Scoring** - Weighted combination ($\\alpha$ $\\cdot$ QFS + $\\beta$ $\\cdot$ DGS + $\\gamma$ $\\cdot$ IFD)\n",
                "\n",
                "## **Metrics Tracked**\n",
                "- Model initialization time\n",
                "- Inference time per frame\n",
                "- Memory usage\n",
                "- Score quality/distribution"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Setup**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Available strategies: ['clip', 'mclip', 'detection', 'combined']\n",
                        "Available YOLO models: ['yolo11n_road_lane', 'yolo11n_bdd100k', 'yolo11l_road_lane', 'yolo11l_bdd100k', 'yolo11n_unified', 'yolo11l_unified']\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import time\n",
                "import gc\n",
                "import psutil\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "from typing import List, Dict, Any, Optional\n",
                "from dataclasses import dataclass, field\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import cv2\n",
                "\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "from src.perception.frame_scorer import (\n",
                "    FrameScorer,\n",
                "    ScoringConfig,\n",
                "    FrameScore,\n",
                "    CLIPScoringStrategy,\n",
                "    MultilingualCLIPScoringStrategy,\n",
                "    DetectionScoringStrategy,\n",
                "    DistinctivenessStrategy,\n",
                "    get_available_strategies,\n",
                "    create_scorer,\n",
                ")\n",
                "\n",
                "from config.settings import get_path_config\n",
                "from src.perception.query_analyzer import QueryAnalyzer\n",
                "from src.perception.model_registry import ModelRegistry, get_model\n",
                "\n",
                "print(f\"Available strategies: {get_available_strategies()}\")\n",
                "print(f\"Available YOLO models: {ModelRegistry.list_models()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Data Preparation**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using test video: 001a9a8b_340_clip_006_0037_0046_Y.mp4\n"
                    ]
                }
            ],
            "source": [
                "paths = get_path_config()\n",
                "train_videos_dir = paths.train_videos_dir\n",
                "video_files = list(train_videos_dir.glob(\"*.mp4\"))[:3]\n",
                "\n",
                "TEST_VIDEO = str(video_files[0])\n",
                "print(f\"Using test video: {Path(TEST_VIDEO).name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracted 16 frames\n",
                        "Frame shape: (1440, 2560, 3)\n"
                    ]
                }
            ],
            "source": [
                "def extract_frames(video_path: str, num_frames: int = 16) -> List[np.ndarray]:\n",
                "    \"\"\"Extract frames uniformly from video.\"\"\"\n",
                "    cap = cv2.VideoCapture(video_path)\n",
                "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    \n",
                "    # Sample uniformly\n",
                "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
                "    \n",
                "    frames = []\n",
                "    for idx in indices:\n",
                "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
                "        ret, frame = cap.read()\n",
                "        if ret:\n",
                "            # Convert BGR to RGB\n",
                "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "            frames.append(frame_rgb)\n",
                "    \n",
                "    cap.release()\n",
                "    return frames\n",
                "\n",
                "# Extract test frames\n",
                "TEST_FRAMES = extract_frames(TEST_VIDEO, num_frames=16)\n",
                "print(f\"Extracted {len(TEST_FRAMES)} frames\")\n",
                "print(f\"Frame shape: {TEST_FRAMES[0].shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Questions with Target Objects:\n",
                        "\n",
                        "1. Biá»ƒn bÃ¡o tá»‘c Ä‘á»™ tá»‘i Ä‘a lÃ  bao nhiÃªu?\n",
                        "   Target Objects: ['traffic_sign', 'max_speed_sign', 'speed_limit_sign']\n",
                        "   YOLO Classes: ['traffic sign']\n",
                        "\n",
                        "2. CÃ³ Ä‘Ã¨n Ä‘á» trong video khÃ´ng?\n",
                        "   Target Objects: ['red_light', 'traffic_light_red']\n",
                        "   YOLO Classes: ['traffic light']\n",
                        "\n",
                        "3. Xe cÃ³ Ä‘Æ°á»£c phÃ©p ráº½ trÃ¡i khÃ´ng?\n",
                        "   Target Objects: ['left_turn', 'direction_left']\n",
                        "   YOLO Classes: ['traffic sign']\n",
                        "\n",
                        "4. CÃ³ bao nhiÃªu lÃ n Ä‘Æ°á»ng?\n",
                        "   Target Objects: ['lane', 'lane_marking']\n",
                        "   YOLO Classes: ['road']\n",
                        "\n",
                        "5. Biá»ƒn cáº¥m nÃ o xuáº¥t hiá»‡n trong video?\n",
                        "   Target Objects: ['prohibitory_sign']\n",
                        "   YOLO Classes: ['traffic sign']\n"
                    ]
                }
            ],
            "source": [
                "# Test questions (Vietnamese)\n",
                "TEST_QUESTIONS = [\n",
                "    \"Biá»ƒn bÃ¡o tá»‘c Ä‘á»™ tá»‘i Ä‘a lÃ  bao nhiÃªu?\",\n",
                "    \"CÃ³ Ä‘Ã¨n Ä‘á» trong video khÃ´ng?\",\n",
                "    \"Xe cÃ³ Ä‘Æ°á»£c phÃ©p ráº½ trÃ¡i khÃ´ng?\",\n",
                "    \"CÃ³ bao nhiÃªu lÃ n Ä‘Æ°á»ng?\",\n",
                "    \"Biá»ƒn cáº¥m nÃ o xuáº¥t hiá»‡n trong video?\",\n",
                "]\n",
                "\n",
                "# Use query analyzer to get target objects\n",
                "analyzer = QueryAnalyzer(strategy=\"keyword\")\n",
                "\n",
                "print(\"Test Questions with Target Objects:\")\n",
                "\n",
                "for i, q in enumerate(TEST_QUESTIONS):\n",
                "    result = analyzer.analyze(q)\n",
                "    print(f\"\\n{i+1}. {q}\")\n",
                "    print(f\"   Target Objects: {result.target_objects[:3]}\")\n",
                "    print(f\"   YOLO Classes: {result.yolo_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Benchmark Utilities**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class BenchmarkResult:\n",
                "    \"\"\"Result from benchmarking a scorer.\"\"\"\n",
                "    strategy_name: str\n",
                "    model_name: str = \"\"\n",
                "    init_time_ms: float = 0.0\n",
                "    avg_inference_ms: float = 0.0\n",
                "    total_inference_ms: float = 0.0\n",
                "    memory_mb: float = 0.0\n",
                "    avg_score: float = 0.0\n",
                "    score_std: float = 0.0\n",
                "    min_score: float = 0.0\n",
                "    max_score: float = 0.0\n",
                "    scores: np.ndarray = field(default_factory=lambda: np.array([]))\n",
                "    success: bool = True\n",
                "    error: str = \"\"\n",
                "\n",
                "\n",
                "def get_memory_mb() -> float:\n",
                "    \"\"\"Get current process memory in MB.\"\"\"\n",
                "    return psutil.Process().memory_info().rss / (1024 * 1024)\n",
                "\n",
                "all_results: List[BenchmarkResult] = []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. **CLIP Scoring Strategy Benchmark**\n",
                "\n",
                "Testing OpenAI CLIP models for Question-Frame Similarity (QFS)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing CLIP model: ViT-B/32\n",
                        "Error: module 'clip' has no attribute 'load'\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'scorer' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     73\u001b[39m     clip_results.append(BenchmarkResult(\n\u001b[32m     74\u001b[39m         strategy_name=\u001b[33m\"\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m         model_name=model_name,\n\u001b[32m     76\u001b[39m         success=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     77\u001b[39m         error=\u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m     78\u001b[39m     ))\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mscorer\u001b[49m\n\u001b[32m     82\u001b[39m gc.collect()\n",
                        "\u001b[31mNameError\u001b[39m: name 'scorer' is not defined"
                    ]
                }
            ],
            "source": [
                "# CLIP models to test\n",
                "CLIP_MODELS = [\n",
                "    \"ViT-B/32\",\n",
                "    \"ViT-L/14\",\n",
                "]\n",
                "\n",
                "clip_results = []\n",
                "test_question = TEST_QUESTIONS[0]\n",
                "\n",
                "for model_name in CLIP_MODELS:\n",
                "    print(f\"Testing CLIP model: {model_name}\")\n",
                "    \n",
                "    gc.collect()\n",
                "    mem_before = get_memory_mb()\n",
                "    \n",
                "    try:\n",
                "        # Initialize\n",
                "        init_start = time.perf_counter()\n",
                "        scorer = CLIPScoringStrategy(\n",
                "            model_name=model_name,\n",
                "            device=\"cpu\",\n",
                "            use_translation=True\n",
                "        )\n",
                "        init_time = (time.perf_counter() - init_start) * 1000\n",
                "        \n",
                "        mem_after = get_memory_mb()\n",
                "        memory_used = mem_after - mem_before\n",
                "        \n",
                "        print(f\"  Init time: {init_time:.2f} ms\")\n",
                "        print(f\"  Memory: {memory_used:.1f} MB\")\n",
                "        \n",
                "        # Warmup\n",
                "        _ = scorer.score(TEST_FRAMES[:2], test_question)\n",
                "        \n",
                "        # Benchmark inference\n",
                "        inference_times = []\n",
                "        all_scores = []\n",
                "        \n",
                "        for q in TEST_QUESTIONS:\n",
                "            start = time.perf_counter()\n",
                "            scores = scorer.score(TEST_FRAMES, q)\n",
                "            elapsed = (time.perf_counter() - start) * 1000\n",
                "            inference_times.append(elapsed)\n",
                "            all_scores.append(scores)\n",
                "        \n",
                "        avg_scores = np.mean([s.mean() for s in all_scores])\n",
                "        avg_time = np.mean(inference_times)\n",
                "        \n",
                "        result = BenchmarkResult(\n",
                "            strategy_name=\"clip\",\n",
                "            model_name=model_name,\n",
                "            init_time_ms=init_time,\n",
                "            avg_inference_ms=avg_time,\n",
                "            total_inference_ms=sum(inference_times),\n",
                "            memory_mb=memory_used,\n",
                "            avg_score=avg_scores,\n",
                "            score_std=np.std([s.mean() for s in all_scores]),\n",
                "            min_score=np.min([s.min() for s in all_scores]),\n",
                "            max_score=np.max([s.max() for s in all_scores]),\n",
                "            scores=all_scores[0],\n",
                "            success=True\n",
                "        )\n",
                "        \n",
                "        print(f\"  Avg inference: {avg_time:.2f} ms for {len(TEST_FRAMES)} frames\")\n",
                "        print(f\"  Per-frame time: {avg_time/len(TEST_FRAMES):.2f} ms\")\n",
                "        print(f\"  Avg score: {avg_scores:.3f}\")\n",
                "        \n",
                "        clip_results.append(result)\n",
                "        all_results.append(result)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        clip_results.append(BenchmarkResult(\n",
                "            strategy_name=\"clip\",\n",
                "            model_name=model_name,\n",
                "            success=False,\n",
                "            error=str(e)\n",
                "        ))\n",
                "    \n",
                "    # Clean up\n",
                "    del scorer\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize CLIP scores\n",
                "if clip_results and any(r.success for r in clip_results):\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Score distribution per model\n",
                "    ax1 = axes[0]\n",
                "    for result in clip_results:\n",
                "        if result.success and len(result.scores) > 0:\n",
                "            ax1.plot(range(len(result.scores)), result.scores, \n",
                "                     marker='o', label=result.model_name)\n",
                "    ax1.set_xlabel('Frame Index')\n",
                "    ax1.set_ylabel('QFS Score')\n",
                "    ax1.set_title(f'CLIP Scores: \"{TEST_QUESTIONS[0][:40]}...\"')\n",
                "    ax1.legend()\n",
                "    ax1.set_ylim(0, 1)\n",
                "    \n",
                "    # Timing comparison\n",
                "    ax2 = axes[1]\n",
                "    models = [r.model_name for r in clip_results if r.success]\n",
                "    times = [r.avg_inference_ms for r in clip_results if r.success]\n",
                "    colors = sns.color_palette('husl', len(models))\n",
                "    bars = ax2.bar(models, times, color=colors)\n",
                "    ax2.set_ylabel('Avg Inference Time (ms)')\n",
                "    ax2.set_title('CLIP Model Inference Time')\n",
                "    for bar, val in zip(bars, times):\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
                "                 f'{val:.1f}', ha='center', fontsize=10)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **2. Multilingual CLIP Benchmark**\n",
                "\n",
                "Testing M-CLIP for direct Vietnamese support (no translation needed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"MULTILINGUAL CLIP BENCHMARK\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "gc.collect()\n",
                "mem_before = get_memory_mb()\n",
                "\n",
                "try:\n",
                "    init_start = time.perf_counter()\n",
                "    mclip_scorer = MultilingualCLIPScoringStrategy(device=\"cuda\")\n",
                "    init_time = (time.perf_counter() - init_start) * 1000\n",
                "    \n",
                "    mem_after = get_memory_mb()\n",
                "    memory_used = mem_after - mem_before\n",
                "    \n",
                "    print(f\"Init time: {init_time:.2f} ms\")\n",
                "    print(f\"Memory: {memory_used:.1f} MB\")\n",
                "    \n",
                "    # Benchmark\n",
                "    start = time.perf_counter()\n",
                "    scores = mclip_scorer.score(TEST_FRAMES, TEST_QUESTIONS[0])\n",
                "    inference_time = (time.perf_counter() - start) * 1000\n",
                "    \n",
                "    result = BenchmarkResult(\n",
                "        strategy_name=\"mclip\",\n",
                "        model_name=\"M-CLIP/XLM-Roberta-Large-Vit-L-14\",\n",
                "        init_time_ms=init_time,\n",
                "        avg_inference_ms=inference_time,\n",
                "        memory_mb=memory_used,\n",
                "        avg_score=float(np.mean(scores)),\n",
                "        scores=scores,\n",
                "        success=True\n",
                "    )\n",
                "    all_results.append(result)\n",
                "    \n",
                "    print(f\"Inference time: {inference_time:.2f} ms\")\n",
                "    print(f\"Avg score: {np.mean(scores):.3f}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"âŒ M-CLIP not available or error: {e}\")\n",
                "    print(\"   Install with: pip install multilingual-clip transformers\")\n",
                "    all_results.append(BenchmarkResult(\n",
                "        strategy_name=\"mclip\",\n",
                "        model_name=\"M-CLIP\",\n",
                "        success=False,\n",
                "        error=str(e)\n",
                "    ))\n",
                "\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Detection Scoring Strategy Benchmark\n",
                "\n",
                "Testing YOLO-based detection boost with different models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"DETECTION SCORING STRATEGY BENCHMARK\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# YOLO models to test\n",
                "YOLO_MODELS = [\n",
                "    \"yolo11n_unified\",\n",
                "    \"yolo11l_unified\",\n",
                "]\n",
                "\n",
                "detection_results = []\n",
                "\n",
                "# Get target classes from question\n",
                "analysis = analyzer.analyze(TEST_QUESTIONS[0])\n",
                "target_classes = analysis.yolo_classes\n",
                "print(f\"\\nTarget classes for '{TEST_QUESTIONS[0][:40]}...': {target_classes}\")\n",
                "\n",
                "for model_name in YOLO_MODELS:\n",
                "    print(f\"\\n{'â”€'*60}\")\n",
                "    print(f\"Testing YOLO model: {model_name}\")\n",
                "    print(f\"{'â”€'*60}\")\n",
                "    \n",
                "    gc.collect()\n",
                "    mem_before = get_memory_mb()\n",
                "    \n",
                "    try:\n",
                "        # Check if model exists\n",
                "        model_info = ModelRegistry.get_model_info(model_name)\n",
                "        if not model_info or not model_info.exists:\n",
                "            print(f\"  âš ï¸ Model weights not found, skipping...\")\n",
                "            continue\n",
                "        \n",
                "        # Initialize\n",
                "        init_start = time.perf_counter()\n",
                "        yolo_model = get_model(model_name, device=\"cuda\")\n",
                "        scorer = DetectionScoringStrategy(confidence=0.25, device=\"cuda\")\n",
                "        scorer.set_model(yolo_model)\n",
                "        init_time = (time.perf_counter() - init_start) * 1000\n",
                "        \n",
                "        mem_after = get_memory_mb()\n",
                "        memory_used = mem_after - mem_before\n",
                "        \n",
                "        print(f\"  Init time: {init_time:.2f} ms\")\n",
                "        print(f\"  Memory: {memory_used:.1f} MB\")\n",
                "        \n",
                "        # Warmup\n",
                "        _ = scorer.score(TEST_FRAMES[:2], TEST_QUESTIONS[0], target_classes)\n",
                "        \n",
                "        # Benchmark\n",
                "        start = time.perf_counter()\n",
                "        scores = scorer.score(TEST_FRAMES, TEST_QUESTIONS[0], target_classes)\n",
                "        inference_time = (time.perf_counter() - start) * 1000\n",
                "        \n",
                "        result = BenchmarkResult(\n",
                "            strategy_name=\"detection\",\n",
                "            model_name=model_name,\n",
                "            init_time_ms=init_time,\n",
                "            avg_inference_ms=inference_time,\n",
                "            memory_mb=memory_used,\n",
                "            avg_score=float(np.mean(scores)),\n",
                "            score_std=float(np.std(scores)),\n",
                "            min_score=float(np.min(scores)),\n",
                "            max_score=float(np.max(scores)),\n",
                "            scores=scores,\n",
                "            success=True\n",
                "        )\n",
                "        detection_results.append(result)\n",
                "        all_results.append(result)\n",
                "        \n",
                "        print(f\"  Inference time: {inference_time:.2f} ms for {len(TEST_FRAMES)} frames\")\n",
                "        print(f\"  Per-frame time: {inference_time/len(TEST_FRAMES):.2f} ms\")\n",
                "        print(f\"  Avg detection score: {np.mean(scores):.3f}\")\n",
                "        print(f\"  Frames with detections: {np.sum(scores > 0)} / {len(scores)}\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  âŒ Error: {e}\")\n",
                "        detection_results.append(BenchmarkResult(\n",
                "            strategy_name=\"detection\",\n",
                "            model_name=model_name,\n",
                "            success=False,\n",
                "            error=str(e)\n",
                "        ))\n",
                "    \n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\nâœ… Tested {len([r for r in detection_results if r.success])} YOLO models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Distinctiveness Strategy Benchmark\n",
                "\n",
                "Testing Inter-Frame Distinctiveness (IFD) scoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"DISTINCTIVENESS STRATEGY BENCHMARK\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Test different window sizes\n",
                "WINDOW_SIZES = [1, 3, 5]\n",
                "\n",
                "ifd_results = []\n",
                "\n",
                "for window_size in WINDOW_SIZES:\n",
                "    print(f\"\\nTesting window_size={window_size}\")\n",
                "    \n",
                "    gc.collect()\n",
                "    \n",
                "    try:\n",
                "        scorer = DistinctivenessStrategy(window_size=window_size)\n",
                "        \n",
                "        # Benchmark\n",
                "        start = time.perf_counter()\n",
                "        scores = scorer.compute_from_frames(TEST_FRAMES)\n",
                "        inference_time = (time.perf_counter() - start) * 1000\n",
                "        \n",
                "        result = BenchmarkResult(\n",
                "            strategy_name=\"distinctiveness\",\n",
                "            model_name=f\"window={window_size}\",\n",
                "            init_time_ms=0.0,\n",
                "            avg_inference_ms=inference_time,\n",
                "            avg_score=float(np.mean(scores)),\n",
                "            score_std=float(np.std(scores)),\n",
                "            min_score=float(np.min(scores)),\n",
                "            max_score=float(np.max(scores)),\n",
                "            scores=scores,\n",
                "            success=True\n",
                "        )\n",
                "        ifd_results.append(result)\n",
                "        all_results.append(result)\n",
                "        \n",
                "        print(f\"  Inference time: {inference_time:.3f} ms\")\n",
                "        print(f\"  Avg distinctiveness: {np.mean(scores):.3f}\")\n",
                "        print(f\"  Most distinct frame: {np.argmax(scores)} (score={np.max(scores):.3f})\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  âŒ Error: {e}\")\n",
                "\n",
                "print(f\"\\nâœ… Tested {len(ifd_results)} window sizes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize distinctiveness scores\n",
                "if ifd_results:\n",
                "    fig, ax = plt.subplots(figsize=(12, 5))\n",
                "    \n",
                "    for result in ifd_results:\n",
                "        ax.plot(range(len(result.scores)), result.scores, \n",
                "                marker='o', label=result.model_name, linewidth=2)\n",
                "    \n",
                "    ax.set_xlabel('Frame Index')\n",
                "    ax.set_ylabel('Distinctiveness Score (IFD)')\n",
                "    ax.set_title('Inter-Frame Distinctiveness by Window Size')\n",
                "    ax.legend()\n",
                "    ax.set_ylim(0, 1)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Combined FrameScorer Benchmark\n",
                "\n",
                "Testing the combined scorer with different weight configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"COMBINED FRAMESCORER BENCHMARK\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Different weight configurations to test\n",
                "WEIGHT_CONFIGS = [\n",
                "    {\"name\": \"QFS Only\", \"alpha\": 1.0, \"beta\": 0.0, \"gamma\": 0.0},\n",
                "    {\"name\": \"Balanced\", \"alpha\": 0.5, \"beta\": 0.3, \"gamma\": 0.2},\n",
                "    {\"name\": \"Detection Focus\", \"alpha\": 0.3, \"beta\": 0.5, \"gamma\": 0.2},\n",
                "    {\"name\": \"Diversity Focus\", \"alpha\": 0.4, \"beta\": 0.2, \"gamma\": 0.4},\n",
                "]\n",
                "\n",
                "combined_results = []\n",
                "\n",
                "for config in WEIGHT_CONFIGS:\n",
                "    print(f\"\\n{'â”€'*60}\")\n",
                "    print(f\"Testing: {config['name']} (Î±={config['alpha']}, Î²={config['beta']}, Î³={config['gamma']})\")\n",
                "    print(f\"{'â”€'*60}\")\n",
                "    \n",
                "    gc.collect()\n",
                "    mem_before = get_memory_mb()\n",
                "    \n",
                "    try:\n",
                "        # Create scorer\n",
                "        init_start = time.perf_counter()\n",
                "        scorer = create_scorer(\n",
                "            strategy=\"combined\",\n",
                "            clip_model=\"ViT-B/32\",  # Use smaller model for combined tests\n",
                "            device=\"cuda\",\n",
                "            use_translation=True,\n",
                "            alpha=config['alpha'],\n",
                "            beta=config['beta'],\n",
                "            gamma=config['gamma']\n",
                "        )\n",
                "        \n",
                "        # Set YOLO model if detection is enabled\n",
                "        if config['beta'] > 0:\n",
                "            try:\n",
                "                yolo_model = get_model(\"yolo11n_unified\", device=\"cuda\")\n",
                "                scorer.set_yolo_model(yolo_model)\n",
                "            except:\n",
                "                print(\"  âš ï¸ YOLO model not available, detection scoring disabled\")\n",
                "        \n",
                "        init_time = (time.perf_counter() - init_start) * 1000\n",
                "        mem_after = get_memory_mb()\n",
                "        \n",
                "        print(f\"  Init time: {init_time:.2f} ms\")\n",
                "        print(f\"  Memory: {mem_after - mem_before:.1f} MB\")\n",
                "        \n",
                "        # Get target classes\n",
                "        analysis = analyzer.analyze(TEST_QUESTIONS[0])\n",
                "        \n",
                "        # Benchmark with detailed scores\n",
                "        start = time.perf_counter()\n",
                "        detailed_scores = scorer.score_frames(\n",
                "            TEST_FRAMES, \n",
                "            TEST_QUESTIONS[0], \n",
                "            target_classes=analysis.yolo_classes,\n",
                "            return_detailed=True\n",
                "        )\n",
                "        inference_time = (time.perf_counter() - start) * 1000\n",
                "        \n",
                "        final_scores = np.array([s.final_score for s in detailed_scores])\n",
                "        qfs_scores = np.array([s.qfs_score for s in detailed_scores])\n",
                "        det_scores = np.array([s.detection_score for s in detailed_scores])\n",
                "        ifd_scores = np.array([s.ifd_score for s in detailed_scores])\n",
                "        \n",
                "        result = BenchmarkResult(\n",
                "            strategy_name=\"combined\",\n",
                "            model_name=config['name'],\n",
                "            init_time_ms=init_time,\n",
                "            avg_inference_ms=inference_time,\n",
                "            memory_mb=mem_after - mem_before,\n",
                "            avg_score=float(np.mean(final_scores)),\n",
                "            score_std=float(np.std(final_scores)),\n",
                "            min_score=float(np.min(final_scores)),\n",
                "            max_score=float(np.max(final_scores)),\n",
                "            scores=final_scores,\n",
                "            success=True\n",
                "        )\n",
                "        combined_results.append(result)\n",
                "        all_results.append(result)\n",
                "        \n",
                "        print(f\"  Inference time: {inference_time:.2f} ms\")\n",
                "        print(f\"  Score breakdown:\")\n",
                "        print(f\"    QFS mean: {np.mean(qfs_scores):.3f}\")\n",
                "        print(f\"    Detection mean: {np.mean(det_scores):.3f}\")\n",
                "        print(f\"    IFD mean: {np.mean(ifd_scores):.3f}\")\n",
                "        print(f\"    Final mean: {np.mean(final_scores):.3f}\")\n",
                "        print(f\"  Top frame: {np.argmax(final_scores)} (score={np.max(final_scores):.3f})\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  âŒ Error: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "    \n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\nâœ… Tested {len(combined_results)} weight configurations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize combined scores\n",
                "if combined_results:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Score distribution per configuration\n",
                "    ax1 = axes[0]\n",
                "    for result in combined_results:\n",
                "        ax1.plot(range(len(result.scores)), result.scores, \n",
                "                 marker='o', label=result.model_name, linewidth=2)\n",
                "    ax1.set_xlabel('Frame Index')\n",
                "    ax1.set_ylabel('Combined Score')\n",
                "    ax1.set_title('Combined Scores by Weight Configuration')\n",
                "    ax1.legend()\n",
                "    ax1.set_ylim(0, 1)\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Bar chart of average scores\n",
                "    ax2 = axes[1]\n",
                "    names = [r.model_name for r in combined_results]\n",
                "    avgs = [r.avg_score for r in combined_results]\n",
                "    colors = sns.color_palette('husl', len(names))\n",
                "    bars = ax2.bar(range(len(names)), avgs, color=colors)\n",
                "    ax2.set_xticks(range(len(names)))\n",
                "    ax2.set_xticklabels(names, rotation=45, ha='right')\n",
                "    ax2.set_ylabel('Average Score')\n",
                "    ax2.set_title('Average Score by Configuration')\n",
                "    for bar, val in zip(bars, avgs):\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
                "                 f'{val:.3f}', ha='center', fontsize=9)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Complete Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary dataframe\n",
                "summary_data = []\n",
                "for r in all_results:\n",
                "    summary_data.append({\n",
                "        \"Strategy\": r.strategy_name,\n",
                "        \"Model/Config\": r.model_name,\n",
                "        \"Init Time (ms)\": f\"{r.init_time_ms:.1f}\",\n",
                "        \"Inference (ms)\": f\"{r.avg_inference_ms:.1f}\",\n",
                "        \"Memory (MB)\": f\"{r.memory_mb:.1f}\",\n",
                "        \"Avg Score\": f\"{r.avg_score:.3f}\",\n",
                "        \"Score Std\": f\"{r.score_std:.3f}\" if r.score_std > 0 else \"-\",\n",
                "        \"Success\": \"âœ…\" if r.success else \"âŒ\"\n",
                "    })\n",
                "\n",
                "df_summary = pd.DataFrame(summary_data)\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"COMPLETE BENCHMARK SUMMARY\")\n",
                "print(\"=\"*100)\n",
                "display(df_summary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Grand visualization\n",
                "successful_results = [r for r in all_results if r.success]\n",
                "\n",
                "if successful_results:\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "    fig.suptitle('Frame Scorer Benchmark Results', fontsize=16, fontweight='bold')\n",
                "    \n",
                "    labels = [f\"{r.strategy_name}\\n({r.model_name[:15]}...)\" if len(r.model_name) > 15\n",
                "              else f\"{r.strategy_name}\\n({r.model_name})\" for r in successful_results]\n",
                "    colors = sns.color_palette('husl', len(successful_results))\n",
                "    \n",
                "    # 1. Init Time\n",
                "    ax1 = axes[0, 0]\n",
                "    init_times = [r.init_time_ms for r in successful_results]\n",
                "    bars = ax1.bar(range(len(successful_results)), init_times, color=colors)\n",
                "    ax1.set_xticks(range(len(successful_results)))\n",
                "    ax1.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
                "    ax1.set_ylabel('Time (ms)')\n",
                "    ax1.set_title('Initialization Time')\n",
                "    ax1.set_yscale('log')\n",
                "    \n",
                "    # 2. Inference Time\n",
                "    ax2 = axes[0, 1]\n",
                "    inf_times = [r.avg_inference_ms for r in successful_results]\n",
                "    bars = ax2.bar(range(len(successful_results)), inf_times, color=colors)\n",
                "    ax2.set_xticks(range(len(successful_results)))\n",
                "    ax2.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
                "    ax2.set_ylabel('Time (ms)')\n",
                "    ax2.set_title('Inference Time (per batch)')\n",
                "    \n",
                "    # 3. Memory Usage\n",
                "    ax3 = axes[1, 0]\n",
                "    memory = [r.memory_mb for r in successful_results]\n",
                "    bars = ax3.bar(range(len(successful_results)), memory, color=colors)\n",
                "    ax3.set_xticks(range(len(successful_results)))\n",
                "    ax3.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
                "    ax3.set_ylabel('Memory (MB)')\n",
                "    ax3.set_title('Memory Usage')\n",
                "    \n",
                "    # 4. Average Score\n",
                "    ax4 = axes[1, 1]\n",
                "    avg_scores = [r.avg_score for r in successful_results]\n",
                "    bars = ax4.bar(range(len(successful_results)), avg_scores, color=colors)\n",
                "    ax4.set_xticks(range(len(successful_results)))\n",
                "    ax4.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
                "    ax4.set_ylabel('Score')\n",
                "    ax4.set_title('Average Score')\n",
                "    ax4.set_ylim(0, 1)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../outputs/frame_scorer_benchmark.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nðŸ“ Chart saved to: outputs/frame_scorer_benchmark.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"ðŸ“Š RECOMMENDATIONS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "if successful_results:\n",
                "    # Find best performers\n",
                "    fastest_init = min(successful_results, key=lambda x: x.init_time_ms if x.init_time_ms > 0 else float('inf'))\n",
                "    fastest_inference = min(successful_results, key=lambda x: x.avg_inference_ms)\n",
                "    highest_score = max(successful_results, key=lambda x: x.avg_score)\n",
                "    lowest_memory = min([r for r in successful_results if r.memory_mb > 0], \n",
                "                        key=lambda x: x.memory_mb, default=None)\n",
                "    \n",
                "    print(f\"\\nðŸš€ Fastest Initialization:\")\n",
                "    print(f\"   {fastest_init.strategy_name}/{fastest_init.model_name}: {fastest_init.init_time_ms:.2f} ms\")\n",
                "    \n",
                "    print(f\"\\nâš¡ Fastest Inference:\")\n",
                "    print(f\"   {fastest_inference.strategy_name}/{fastest_inference.model_name}: {fastest_inference.avg_inference_ms:.2f} ms\")\n",
                "    \n",
                "    print(f\"\\nðŸŽ¯ Highest Score:\")\n",
                "    print(f\"   {highest_score.strategy_name}/{highest_score.model_name}: {highest_score.avg_score:.3f}\")\n",
                "    \n",
                "    if lowest_memory:\n",
                "        print(f\"\\nðŸ’¾ Lowest Memory:\")\n",
                "        print(f\"   {lowest_memory.strategy_name}/{lowest_memory.model_name}: {lowest_memory.memory_mb:.1f} MB\")\n",
                "\n",
                "print(\"\\n\" + \"â”€\"*80)\n",
                "print(\"\\nðŸ“‹ USE CASE RECOMMENDATIONS:\")\n",
                "print(\"\\n  â€¢ Real-time (low latency): Use CLIP ViT-B/32 with Î±=1.0 (QFS only)\")\n",
                "print(\"  â€¢ Best quality: Use CLIP ViT-L/14 with balanced weights\")\n",
                "print(\"  â€¢ Object-specific queries: Enable detection scoring (Î² > 0)\")\n",
                "print(\"  â€¢ Reduce redundancy: Enable distinctiveness (Î³ > 0)\")\n",
                "print(\"  â€¢ Vietnamese support: Use translation=True (recommended) or M-CLIP\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to CSV\n",
                "export_data = []\n",
                "for r in all_results:\n",
                "    export_data.append({\n",
                "        \"strategy\": r.strategy_name,\n",
                "        \"model\": r.model_name,\n",
                "        \"init_time_ms\": r.init_time_ms,\n",
                "        \"avg_inference_ms\": r.avg_inference_ms,\n",
                "        \"memory_mb\": r.memory_mb,\n",
                "        \"avg_score\": r.avg_score,\n",
                "        \"score_std\": r.score_std,\n",
                "        \"min_score\": r.min_score,\n",
                "        \"max_score\": r.max_score,\n",
                "        \"success\": r.success,\n",
                "        \"error\": r.error\n",
                "    })\n",
                "\n",
                "export_df = pd.DataFrame(export_data)\n",
                "export_df.to_csv('../outputs/frame_scorer_benchmark.csv', index=False)\n",
                "\n",
                "print(\"âœ… Results exported to: outputs/frame_scorer_benchmark.csv\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"BENCHMARK COMPLETE\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nTotal strategies tested: {len(all_results)}\")\n",
                "print(f\"Successful: {len([r for r in all_results if r.success])}\")\n",
                "print(f\"Failed: {len([r for r in all_results if not r.success])}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
